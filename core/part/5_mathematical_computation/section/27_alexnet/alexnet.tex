\section{2012 – The Deep Learning Revolution: AlexNet}

By the late 2000s, neural networks had matured.  
CNNs were effective at vision.  
RNNs (with LSTMs) handled sequences.  
But the field still felt... constrained.  
Progress was slow. Datasets were small. Training was expensive.  
Many believed neural networks had hit a ceiling.

Then, in 2012, everything changed.

\textbf{AlexNet}, a deep convolutional neural network designed by \textbf{Alex Krizhevsky}, \textbf{Ilya Sutskever}, and \textbf{Geoffrey Hinton}, didn’t just advance the state of the art—it \emph{redefined the art entirely}.

\subsection{The Catalyst: ImageNet}

At the heart of the revolution was the \textbf{ImageNet Large Scale Visual Recognition Challenge (ILSVRC)}: a brutal test of real-world AI, where models had to classify 1.2 million high-resolution images into 1,000 categories—everything from goldfish to golf carts.

For years, computer vision systems had chipped away at the error rate using hand-engineered features: SIFT, HOG, edge detectors, and domain-specific pipelines that relied on expert insight.

Then came AlexNet.

\subsection{What Made It Different}

AlexNet didn’t just perform well—it demolished the competition.  
Its top-5 error rate was over \textbf{10 percentage points lower} than the next best model. In machine learning, that’s not a gap—it’s a canyon.

So what changed?

\begin{itemize}
  \item \textbf{Depth:} AlexNet stacked eight layers—five convolutional, three fully connected. Earlier CNNs had maybe two or three layers. This deeper structure allowed it to build richer, more abstract representations.
  
  \item \textbf{ReLU Activation:} Replacing traditional sigmoid or tanh units, AlexNet used the rectified linear unit (ReLU), which sped up training dramatically by avoiding vanishing gradients and introducing sparsity in the activations.
  
  \item \textbf{Dropout:} To prevent overfitting, AlexNet randomly dropped connections during training—a regularization trick that added noise and robustness.
  
  \item \textbf{GPU Acceleration:} Crucially, AlexNet was trained on two NVIDIA GPUs. What would have taken weeks on CPUs now took days. The hardware caught up with the algorithms—and deep learning finally had room to breathe.
\end{itemize}

\subsection{Why It Mattered}

AlexNet didn’t just outperform—it disrupted.  
It proved that deep neural networks, if scaled properly and trained on large data, could \textbf{learn representations better than humans could engineer them}. No more handcrafted edge filters. No more guesswork. The network found the features for you.

Overnight, neural networks stopped being retro curiosities and became the future.

\begin{itemize}
  \item Tech companies retooled their research labs.
  \item Startups pivoted overnight to become “AI-first.”
  \item Researchers shifted en masse from symbolic AI to deep learning.
\end{itemize}

CNNs, once a niche academic curiosity, became the default architecture for vision.  
GPUs, once the domain of gamers and physicists, became indispensable tools for machine learning.  
The language changed, too—“deep learning” became the new banner of AI progress.

\subsection{Legacy: Lighting the Fuse}

AlexNet wasn’t alone for long. It kicked off an arms race:
\begin{itemize}
  \item \textbf{VGGNet (2014)} — deeper and simpler, showing the power of stacking.
  \item \textbf{GoogLeNet (2014)} — introducing the inception module for efficient computation.
  \item \textbf{ResNet (2015)} — conquering vanishing gradients with skip connections, pushing networks over 100 layers deep.
\end{itemize}

But AlexNet will always be remembered as the one that lit the fuse.

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!50!black, title={AlexNet: From Curiosity to Conquest}]
Neural networks were known.  
Backpropagation was known.  
CNNs were known.  

But in 2012, AlexNet did something radical:  
It combined them all—with enough data, depth, and hardware—to finally break through.  

The world had changed.  
And AI would never be the same.
\end{tcolorbox}
