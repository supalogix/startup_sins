\section{From Programming to Learning: Rosenblatt’s Perceptron (1958)}

By the late 1950s, computers could do many things—sort data, simulate physics, even play simple games. But one thing they \emph{couldn’t} do was learn. Every behavior had to be hand-crafted by a programmer, line by painstaking line.

\medskip

\textbf{Enter Frank Rosenblatt.}

A psychologist by training, Rosenblatt wasn’t trying to build a calculator. He was trying to build a brain—or at least, something that could mimic its ability to learn from experience. His answer was the \textbf{perceptron}: a simple, adaptive system that could classify inputs based on feedback, without being explicitly told how to do it.

\medskip

At a time when computers followed fixed instructions, Rosenblatt proposed a model that \textbf{changed itself} based on outcomes. The perceptron consisted of input nodes connected to an output node through adjustable weights. When given a labeled example (like an image with a known category), the system would:

\begin{enumerate}
  \item Compute a weighted sum of its inputs.
  \item Compare the result to a threshold.
  \item Adjust the weights based on whether it got the answer right or wrong.
\end{enumerate}

\[
y = 
\begin{cases}
1 & \text{if } \sum w_i x_i \geq \theta \\
0 & \text{otherwise}
\end{cases}
\quad \longrightarrow \quad
w_i \gets w_i + \eta (t - y) x_i
\]

Here, \( x_i \) are the inputs, \( w_i \) are the weights, \( \theta \) is the threshold, \( y \) is the output, \( t \) is the true label, and \( \eta \) is the learning rate.

\medskip

\textbf{This was something new.}  
Traditional computers executed logic. The perceptron adjusted itself. It was no longer a program in the traditional sense—it was a system that shaped its own program through interaction with the world.

\medskip

Critics (most famously Minsky and Papert) would later show its limitations. But the perceptron’s deeper legacy wasn’t in its power—it was in its philosophy. It redefined what computation could be: not just execution, but \textbf{adaptation}. Not just following instructions, but \textbf{learning from feedback}.

\begin{quote}
The perceptron was a blueprint for a new kind of machine—one that didn't just compute answers, but \emph{figured out} how to compute them.
\end{quote}


\begin{tcolorbox}[title={\textbf{Historical Sidebar: B.F. Skinner and the Mechanization of Behavior}}, colback=gray!5, colframe=black, fonttitle=\bfseries]

  In the early 20\textsuperscript{th} century, psychology was undergoing a transformation. Introspection and abstract theorizing were out; observable behavior was in. Leading this charge was \textbf{B.F. Skinner}, the iconic behaviorist who believed that the mind was not a black box to be opened, but a black box to be shaped.
  
  Skinner’s central idea was \textbf{operant conditioning}: behaviors could be reinforced or extinguished based on external stimuli. If a pigeon pecks a button and gets food, it will peck again. No thoughts, no feelings—just inputs and outputs. Like a well-trained algorithm, behavior was the product of its environment.
  
  This radical focus on \emph{stimulus-response learning} inspired an entire generation of thinkers—some in psychology, some in engineering. Skinner’s work resonated with early computer scientists and neuroscientists who dreamed of building machines that could learn in the same way animals adapted to their environment.
  
  Enter \textbf{Frank Rosenblatt}. His \textit{perceptron}, built in 1958, was the first model of a learning neural network. Though biologically inspired, the perceptron shared Skinner’s core insight: learning is a \textbf{feedback loop}. Weights (analogous to behavioral tendencies) are adjusted based on success or failure. In this way, the perceptron became a mechanical echo of Skinner’s psychological vision—a machine that learns not by thinking, but by responding.
\end{tcolorbox}

