\subsection{Feedback Loops: Because the Model Thought All Users Were Bots for Two Months}

\textbf{Act I: The Spiral Begins}

At a midsize fintech company, a fraud detection model began quietly flagging nearly every new user as suspicious.

Conversion rates dropped.\\
Support tickets piled up.\\
Engineering blamed data science.\\
Data science blamed data engineering.\\
Marketing just said, ``Something feels off.''

But technically, the model was doing exactly what it was trained to do.

\vspace{1em}
\textbf{Act II: The Innocent Mistake}

One week, a few high-risk accounts slipped through.\\
They weren’t caught in time.

So the feedback loop retrained the model using those users as negative examples.\\
The model learned: ``These are bad.''

Then it flagged more borderline users.\\
Those too were fed back in as negative examples.

The loop tightened. The model hardened.

\vspace{1em}
\textbf{Act III: The Botpocalypse}

Eventually, the model reached its final form:  
Everyone was a bot.

Especially if you had a middle name.\\
Or used a VPN.\\
Or signed up during lunch.

The model wasn’t broken.\\
It was confidently, aggressively wrong.

\vspace{1em}
\textbf{Act IV: The Blame Game}

No one noticed at first.\\
By the time they did, it had been two months.

Support was overwhelmed. Conversions had cratered.\\
And no one could explain how it had gotten so bad — so fast.

The feedback loop had done its job. Just without supervision.

\vspace{1em}
\textbf{Epilogue: Why Feedback Loops Matter}

Without oversight — human or otherwise — your model can spiral into a feedback chamber of its own mistakes.

Label drift, concept drift, and unintended self-reinforcement are real.\\
Your model doesn’t know what it doesn’t know.

If you don’t close the loop with careful monitoring, \textbf{your system will confidently and automatically learn the wrong thing.}



\begin{tcolorbox}[title=Sidebar: Whistleblowers as Rejected Feedback, colback=gray!10, colframe=black, fonttitle=\bfseries]

    In control theory, a system without feedback is a system courting disaster.  
    But in organizations, feedback doesn’t always arrive as tidy data—it often arrives as a memo, a protest, or a whisper.
    
    \textbf{Whistleblowers are feedback loops in human form.}
    
    They observe internal failure modes: unsafe designs, corrupt incentives, ethical contradictions. They generate corrective signals—often early, often urgent. But unlike digital sensors, they can be silenced.
    
    In the case of the 737 MAX, internal engineers raised red flags. FAA officials questioned the delegation of safety oversight. Multiple feedback channels lit up red.  
    And yet—rather than responding—management rerouted the signals, suppressed alerts, and maintained the illusion of stability.
    
    \textbf{In cybernetics, ignoring error signals leads to instability.}  
    \textbf{In corporations, it leads to tragedy.}
    
    \textit{Every ignored whistleblower is a feedback loop severed. Every silenced concern is a system nudging closer to resonance, then collapse.}
    
    So ask yourself:  
    \textbf{Is your organization optimizing performance? Or rejecting feedback?}
    
\end{tcolorbox}