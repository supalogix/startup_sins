\section{Modularity: Because the Intern Deployed a Hardcoded Model to Prod}

\begin{figure}[H]
\centering

% === First row ===
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Engineer 1}
    {Engineer 2}
    {\footnotesize Let's just log it like this for now. I'll document it later.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{Prologue: It begins, as always, with good intentions.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Engineer 1}
    {Engineer 2}
    {\footnotesize v2FlagTemp? Yeah, just set it to TRUE if the system feels unstable.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{The design philosophy: chaos with confidence.}
\end{subfigure}

\vspace{1em}

% === Second row ===
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Executive 1}
    {Executive 2}
    {\footnotesize We’re preparing a strategic reorg. Most of the original team will be gone by Friday.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{The culling: efficiencies must be unlocked.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Executive 1}
    {Executive 2}
    {\footnotesize But I’m not worried. I’ve been assured everything is fully documented.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{The punchline: spoken like someone who’s never used Confluence.}
\end{subfigure}

\caption*{The Birth of a Legacy System: High hopes and zero documentation.}
\end{figure}

\subsection{Act I: The Blessing}

It always starts the same way.

An intern joins a big company: the kind with seventeen internal platforms named after birds, seven ways to deploy code, and exactly zero onboarding documentation that's less than two fiscal years out of date.

A company so deep in perpetual reorgs that entire teams don’t even know which floor they're on anymore—let alone where the bathrooms are. Asking for directions gets you a Slack thread, a deprecated wiki link, and a shrug.

\medskip

\begin{HistoricalSidebar}{The MBA Mindset—When Management Became a Spreadsheet}
    Business schools promised to turn managers into leaders.  

    \medskip

    What they really did was turn leaders into spreadsheet jockeys.

    \medskip
    
    Starting in the postwar boom of the 1950s, \textbf{management theory became an academic discipline}, a way to turn messy human organizations into controllable systems. First it was \textit{Taylorism} and \textit{scientific management}, slicing every task into a stopwatch-measured unit. 

    \medskip
    
    Then came \textit{Six Sigma}, \textit{Lean}, \textit{Lean Six Sigma}, \textit{Agile}, \textit{Scaled Agile Framework} \textit{Disciplined Agile Delivery} --- each one a new operating system for the corporation, each one a promise of smoother flow, fewer defects, higher ROI.

    \medskip
    
    But underneath the jargon lay a simple principle:  

    \begin{quote}
    \textbf{Everything is a process. Everything can be optimized. Everyone is a cell in the workbook.}
    \end{quote}
    
    To the MBA mind, the org chart isn’t a map of people...  it’s a pivot table.  

    \medskip
    
    A “vertical reporting structure” isn’t about hierarchy—it’s about columns vs. rows.  

    \medskip

    A “horizontal reorg” isn’t a cultural shift—it’s a filter operation in Excel.  

    \medskip

    And when you get “consolidated,” it’s just another \texttt{SUM()} function rolling up a few line items.

    \medskip
    
    In this worldview, the company isn’t a living organism.  It’s a spreadsheet looking for a macro.
    
    \begin{quote}
    \textit{“Management isn’t about leading people. It’s about formatting cells.”}
    \end{quote}
\end{HistoricalSidebar}

\medskip

Their mission? “Help out with a compliance thing.”

Here’s what happened: the Chief Compliance Officer had swung by engineering earlier that week holding a printout from some internal audit checklist. “We’re supposed to have ‘automated anomaly detection’ on user session logs for SOC-2 subsection 4.3.1 compliance,” she’d said, reading the words like they were spells in an unfamiliar language. “We don’t actually need alerts or dashboards or anything. We just need to show we *have* something.”

The lead engineer had nodded solemnly, understanding immediately: this wasn’t about detecting anomalies. This was about **checking a box** so the auditors would go away.

And he had just the solution.

“Let’s give it to the intern,” he said.

After all, it wasn’t a revenue feature. It wasn’t security-critical. It wasn’t even a real model request—it was compliance theater: a binary classifier whose mere existence was enough to satisfy a requirement nobody cared to interpret too closely.

The rule itself—SOC-2 subsection 4.3.1—was a vague line item about “proactive identification of anomalous user activity.” Not fraud detection. Not real-time monitoring. Just some plausible process that could be waved at an auditor and described with buzzwords like “machine learning” and “automation.”

The model didn’t need to work. It didn’t need to catch anomalies. It just needed to **exist on paper**.

And yet — paradoxically — this model had to be robust, fast, explainable, and deployable by Friday. It’s the Schrödinger’s cat of machine learning: simultaneously fake and real, invisible and on fire.

Now here’s the catch: no one could tell them where the data came from. Or where it went. Or what shape it was in. Or even what system was writing to it.

All anyone knew was that somewhere—out there—something was emitting telemetry logs.

Not regularly. Not predictably. But reliably enough that every few hours, another log file appeared in a web dashboard no one remembered creating. The dashboard URL was buried inside an old wiki page under “temporary monitoring (delete later)”—last edited four years ago by an account that no longer existed.

\textbf{In other words: institutional memory had evaporated.}

\begin{HistoricalSidebar}{The Lost Knowledge of Apollo}

    During the Apollo era, NASA undertook the monumental task of landing humans on the Moon. This endeavor required the collaboration of thousands of engineers, scientists, and technicians, each contributing specialized knowledge to the mission's success.

    \medskip
    
    One notable figure was \textbf{Margaret Hamilton}, who led the software engineering team responsible for the Apollo Guidance Computer. Her team's work was crucial during the Apollo 11 mission, particularly when unexpected computer alarms threatened to abort the lunar landing. The robustness of their software allowed the mission to proceed safely.

    \medskip
    
    However, much of the intricate knowledge from the Apollo program was never comprehensively documented. As the program wound down and personnel moved on, NASA faced an uncomfortable realization: critical institutional memory was evaporating. Knowledge was dispersed across thousands of minds, each holding only a fragment of the whole. No single person knew how to put a human on the Moon.

    \medskip
    
    Efforts were made to capture what could be salvaged. Engineers interviewed veterans, scoured archives, compiled schematics. But some knowledge was already irretrievably lost. Processes that once depended on tacit understanding, undocumented workarounds, or informal networks of expertise had no paper trail to follow.

    \medskip
    
    It’s a cautionary tale: even a triumph like Apollo can become a legacy system. And once the last person leaves the room, you might find that no one knows how it actually worked.
    
\end{HistoricalSidebar}


So the intern did what anyone would do under the circumstances:



\begin{itemize}
    \item They clicked “Export CSV” from the mystery web dashboard.
    \item They opened the CSV in Jupyter like it was a sacred ritual passed down from Stack Overflow itself.
    \item They grep’ed.
\end{itemize}

And miraculously, it loaded… sort of.

There were nulls in half the columns, timestamps in three different formats, and a mysterious field called \texttt{v2FlagTemp} that no one ever defined.

Nobody knew what \texttt{v2FlagTemp} actually did. Some said it was a deprecation marker. Others claimed it toggled a legacy feature that hadn’t existed since the Kubernetes migration of ’19. One engineer swore it flipped to \texttt{TRUE} during solar flares.

Meanwhile, the timestamps were a small chaos engine unto themselves—sometimes ISO with a `T`, sometimes space-separated, sometimes slashed Y/M/D.

Parsing the logs felt less like data engineering and more like digital paleontology. You weren’t cleaning data. No, you were performing forensic analysis on a telemetry feed no one remembered enabling.

\begin{lstlisting}[
    caption={Sample telemetry logs exported from the mystery dashboard},
    label={lst:ancientlogs},
    basicstyle=\ttfamily\small,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    breaklines=true,
    breakatwhitespace=false,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
  ]
2020-01-03T10:15:32Z level=INFO deviceId=node12 metricA=12.3 metricB=7.1 target=null v2FlagTemp=TRUE
2020-01-03 10:15:33 INFO deviceId=node12 metricA=13.1 metricB=8.0 target=4.0
2020/01/03 10:15:34 INFO deviceId=node12 metricA=nan metricB=6.9 target=null
2020-01-03 10:15:35 WARN deviceId=node17 v2FlagTemp=FALSE droppedPackets=5
2020-01-03T10:15:36Z level=INFO deviceId=node12 metricA=11.8 metricB=7.3 target=3.9 v2FlagTemp=1
2020-01-03T10:15:37Z level=ERROR deviceId=??? metricA=? metricB=? target=missing
2020-01-03 10:15:38 INFO deviceId=node12 metricA=13.0 metricB=8.1 target=4.1
\end{lstlisting}

But the intern pressed on because a few \texttt{dropna()} calls and some light string parsing would bring order to the chaos. Hope, after all, is the most dangerous optimizer.

\begin{HistoricalSidebar}{“Sort of Working” and the Robustness Principle}

    Software engineers have a term for this:  

    \begin{quote}
    “It sort of works.”
    \end{quote}
    
    Not flawless. Not elegant. But working—enough.

    \medskip
    
    This mindset traces back to an idea known as the \textbf{Robustness Principle}, coined by Jon Postel in 1981 during the early days of the Internet:  

    \begin{quote}
    \textit{“Be conservative in what you do, be liberal in what you accept from others.”}
    \end{quote}
    
    In essence: tolerate imperfect inputs, handle edge cases gracefully, and keep the system moving forward even when things don’t quite fit.  

    \medskip

    A philosophy of resilience over rigidity.  

    \medskip

    A quiet acknowledgment that in a messy, interconnected world, sometimes “sort of working” is good enough—because it keeps working.
    
    \medskip
    
    But here’s the twist: the same principle applies outside of software.

    \medskip
    
    In interpersonal relationships, it’s often said:  

    \begin{quote}
    \textit{“It’s better to be 50\% present 100\% of the time than 100\% present 50\% of the time.”}
    \end{quote}
    
    Why?  Because what people value most isn’t perfection.  It’s \textbf{reliability}.  

    \medskip

    If someone truly appreciates your company, they’d rather have half of you, consistently, than all of you, sporadically.

    \medskip
    
    The same is true of program features:  A clunky tool that’s always there beats a perfect tool that disappears when you need it most.
    
\end{HistoricalSidebar}

Armed with Stack Overflow tabs and unshakable faith in pandas, they began the sacred rite of modern data cleaning: (a) removing nulls without asking why they were there, (b) coercing all timestamps into ISO format (with silent fallback), and (c) mapping suspicious Boolean-like strings to something vaguely coherent. \texttt{v2FlagTemp}? Treated as a categorical, obviously. Unused features? Dropped with the righteous confidence of someone who had never seen a postmortem. \textbf{Because nothing says “production ready” like blindly transforming orphaned telemetry logs scraped from an unknown source.}

Fresh off the intoxicating high of successfully reading a CSV, the intern embarked on their next great odyssey.

They did what every unsupervised intern with vague requirements, no staging environment, and a deeply misplaced sense of destiny inevitably does: they wrote a script.

Not a pipeline. Not a collection of well-documented, reusable modules.

No. This was different.

It was a single, sprawling, glorious file—an unbroken stream of code that took data from manual download to manual upload. From CSV import to model export to celebratory print statement. All in one script. All in one breath.

\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{teal},
  breaklines=true,
  breakatwhitespace=false,
  postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space},
  showstringspaces=false,
  frame=single,
  caption={The Monolith Script in All Its Glory},
  label={lst:monolith},
  numbers=left,
  numberstyle=\tiny,
  language=Python
}

\begin{lstlisting}[language=Python]
# the_monolith.py

import pandas as pd
import pickle
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Step 1: Load exported CSV manually downloaded from dashboard
csv_path = "/Users/intern/Downloads/mystery_telemetry_export.csv"
df = pd.read_csv(csv_path, sep=r'\s+', engine='python', error_bad_lines=False)

# Step 2: Parse columns using regex-like string splits
df['metricA'] = pd.to_numeric(df['metricA'], errors='coerce')
df['metricB'] = pd.to_numeric(df['metricB'], errors='coerce')
df['target'] = pd.to_numeric(df['target'], errors='coerce')

# Drop rows where target or features are missing
df = df.dropna(subset=['metricA', 'metricB', 'target']).reset_index(drop=True)

# Step 3: Train model
X = df[['metricA', 'metricB']]
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = LinearRegression()
model.fit(X_train, y_train)

# Step 4: Save model
with open("model.pkl", "wb") as f:
    pickle.dump(model, f)

# Step 5: Upload manually to compliance dashboard
print("Upload 'model.pkl' to compliance-portal.internal/upload and check the box manually.")

print("Model training complete. Nothing could possibly go wrong.")
\end{lstlisting}

It pulled packages from three different ecosystems, used regex-as-a-strategy, and had print statements doubling as deployment documentation.

There were no functions… only faith.  
There was no error handling… only optimism.

The monolith had been born.

\begin{HistoricalSidebar}{“Monolith”—From Stone to Software}

The word \textbf{monolith} traces back to the Ancient Greek \textit{monolithos}, from \textit{monos} (single) + \textit{lithos} (stone):  
\begin{quote}
\textit{“A single massive block of stone.”}
\end{quote}

Think Stonehenge. Think Easter Island. Think towering obelisks carved from one uninterrupted slab.  
A monument that is solid, unified, and immovable.

It wasn’t until the 20th century that engineers began borrowing the term for other domains: giant corporate structures, massive bureaucracies, or anything unwieldy and indivisible.

By the time it arrived in software engineering, \textbf{“monolith”} described applications built as a single, tightly coupled unit:  
No clean seams, no modular components, no easy way to break it apart.

In short:  
\begin{quote}
A structure as awe-inspiring—and as impossible to rearrange—as a stone temple.
\end{quote}

So when the script choked, it wasn’t just a failure.  
It was a monument to an architectural philosophy:  
\textbf{“Build once. Move never.”}
\end{HistoricalSidebar}


It was ugly. It was fragile. It was unreadable to everyone, including the intern who wrote it.

But—miraculously—it worked.

\begin{quote}
And for one brief, cursed moment, the system was in balance.
By the time the Jupyter notebook was done, the data looked “clean.”  
Not correct. Not meaningful. But clean enough to pass a code review;  
and that, in the intern’s defense, was all it needed to do to check the box.
\end{quote}




\subsection{Act II: The Fall}

Weeks passed.

No one touched the script. No one asked about the model.

It sat in its digital corner, whirring quietly inside a Docker container no one dared to rebuild, delivering predictions with the eerie consistency of a haunted Roomba—faithful, silent, and faintly menacing.

\begin{HistoricalSidebar}{“It Works on My Machine”—The Defense That Became Infrastructure}

For as long as software has existed, engineers have wielded a near-perfect excuse for bugs, delays, and failures:  

\begin{quote}
\textbf{It works on my machine.}
\end{quote}

A line so simple, so disarming, it carries just enough plausible deniability to stall accountability.  
How do you prove otherwise? Short of physically visiting the engineer’s desk or replicating every library, path, and environment variable—they win by default.

\medskip

It wasn’t just an excuse. It was a loophole.  
And loopholes scale.

\medskip

Like any unenforceable law, the rule of “software must work everywhere” slowly eroded in the face of practical impossibility. If you can’t enforce a law, the law doesn’t really exist.  
If everyone’s machine is different, then whose machine is “real”?

\medskip

\noindent Enter Docker.

\medskip

Docker didn’t eliminate the loophole. It industrialized it.  
Why fight “it works on my machine” when you can simply \textbf{ship the machine}?

\medskip

A container wasn’t just a runtime—it was a diplomatic treaty between development, testing, and production.  
A frozen snapshot of “my machine,” wrapped and sealed for transit.

\medskip

``It works on my machine'' had gone from punchline to product.  
And in doing so, it solved the problem in the most engineering way possible:  
not by closing the loophole, but by automating it.

\end{HistoricalSidebar}



Until, of course, the day it didn’t.

It was a Friday afternoon. The kind of Friday afternoon when half the team had already mentally clocked out and the other half was pretending to write documentation.

Then it happened.

\textbf{The logs changed.}

Not a dramatic change—just enough to be invisible to humans, but fatal to regex. A new column had quietly appeared, unnamed and smug. One old column, long relied upon, was gone without a trace. The date format had subtly shifted—no longer slashes, but dashes. Or was it the timezone? No one knew. The changes crept in like code gremlins, tiptoeing past CI pipelines.

And the script—poor, unsuspecting monolith that it was—choked.

The model broke.

The service failed.

Metrics flatlined. Dashboards turned a shade redder than anyone was comfortable with.

PagerDuty screamed into the void.

On-call phones vibrated on desks and nightstands with all the fury of a thousand missed deadlines. Slack channels lit up like a Christmas tree in a lightning storm. Messages began with \texttt{"Hey..."} and ended with \texttt{"@here."}

The intern, who had last touched the code three rotations ago, was summoned like a wizard in exile. Somewhere, their laptop opened slowly.

The autopsy hadn’t started, but the blame game had.

\medskip 

\begin{lstlisting}[caption={Slack transcript, 2:43 PM on a Friday}, label={lst:slackpanic}, basicstyle=\ttfamily\small, frame=single]
#sre-oncall

[2:43 PM] @alertbot: [ALERT] PROD Model Service Failure - HTTP 500s detected
[2:44 PM] @senior_sre: who owns this??
[2:44 PM] @eng_manager: wasnt this the interns thing?
[2:45 PM] @intern: hi yes uh give me 5 min
[2:46 PM] @senior_sre: what changed?
[2:46 PM] @intern: the logs have... evolved?
[2:47 PM] @intern: feature2 is now feature3
[2:47 PM] @intern: also the timestamps are in ISO format? I think?
[2:48 PM] @senior_sre: wheres the source?
[2:48 PM] @intern: it is not in Git
[2:48 PM] @senior_sre: what
[2:49 PM] @intern: I think I uploaded it through the command line from my VM last summer?
[2:49 PM] @eng_manager: do we have any docs?
[2:49 PM] @intern: there was a notebook... on my old laptop.
[2:50 PM] @alertbot: [ALERT] PROD Model Service Crash Loop Restarting
[2:51 PM] @senior_sre: this is why we dont YOLO deploy models
[2:51 PM] @intern: understood
[2:52 PM] @eng_manager: rewrite everything
\end{lstlisting}

They searched for the source code.

Desperation mounting, they combed through old repos, abandoned branches, shared drives with names like \texttt{archive\_final\_bkp\_DO\_NOT\_DELETE}. They unzipped tarballs nested within tarballs like digital matryoshka dolls. They even searched Confluence.

They found nothing.

No Git history. No documentation. Not even a rogue Jupyter notebook half-filled with markdown and regret.

They hadn’t deployed a model.  
They had released an unsupervised cryptid into production.

An eldritch pipeline, stitched together by interns long since graduated, silently shaping the fate of a product roadmap.

And now it was angry.

\begin{lstlisting}[caption={The log that broke the build}, label={lst:falllogs}, basicstyle=\ttfamily\small, frame=single]
2020-01-03 10:15:36 INFO  | feature1=13.0 feature2=8.1 target=4.1

% One week later...
2020-01-10 10:15:36 INFO  | feature1=13.0 feature3=foo status=OK timestamp=2020-01-10T10:15:36Z
\end{lstlisting}

\textbf{And that’s when it escalated.}

The executive who’d initially approved the “compliance model” got the call. The lead engineer stood in front of her Slack DM like a soldier awaiting orders.

“Wait,” she typed. “I thought this was handled.”

“Handled,” the engineer confirmed, sweating audibly through the keyboard.

“Then why am I getting a call from the Chief Compliance Officer?”

It turned out the failure had tripped a reporting trigger in the upstream data pipeline. The anomaly detection system—which no one realized was wired into the compliance reporting dashboard—had gone silent.

A compliance auditor had been monitoring the dashboard.

They saw a flatline where a graph was supposed to be.

“Is your anomaly detection down?” the email asked.  “No anomalies for 48 hours seems... unlikely.”

What engineering didn’t realize was that the dashboard wasn’t just a visualization—it was being used as a **compliance log record.** Under SOC-2 section 4.3.1(b), the organization was required to demonstrate “continuous monitoring and evidence of detection capability.”

The flatline wasn’t just “no anomalies.”

The flatline was being interpreted as **“no monitoring.”**

In other words:  \textbf{no anomalies → no alerts → no logs → no proof the system was functioning → failure of the control.}

The VP forwarded the email to the Chief Compliance Officer.

The Chief Compliance Officer forwarded it to Legal.

Legal forwarded it to Risk.

Risk forwarded it to the Board.

The Board requested a full compliance audit.

And somewhere, quietly, a Jira ticket labeled “low priority – intern project” was re-opened with a red flag icon.


\begin{quote}
In other words: a checked box had been unchecked by production reality.
\end{quote}

What started as a token model to placate an audit had become an existential compliance liability.

A third-party auditor was brought in.

\begin{itemize}

    \item They requested documentation.
    \item They requested training data.
    \item They requested logs of inference outputs.
    \item They requested a clear explanation of “feature2” versus “feature3.”
    \item They requested an explanation of the regression thresholds.
    \item They requested the model card.
    \item They requested proof the model even existed.

\end{itemize}

\textbf{There was none.}

The intern had built a monument to hubris.

And now it had become a monument to noncompliance.
    



\subsection{Act III: The Baptism}

    The audit had begun.
    
    Compliance demanded answers.
    
    Legal demanded timelines.
    
    Risk demanded mitigation.
    
    Executives demanded that someone—anyone—“own it.”

    \medskip

\begin{HistoricalSidebar}{The Legacy of SOX: "Whatever You Do, Don't Tell Me"}

    When the Sarbanes-Oxley Act of 2002 (SOX) was passed in the wake of Enron and WorldCom, it didn’t just introduce stricter accounting rules—it rewired executive psychology.

    \medskip
    
    By making the \textbf{CEO and CFO personally liable} for knowingly certifying false financial statements (Sections 302 and 906), SOX created a powerful incentive:

    \medskip
    
    Not to know.

    \medskip
    
    Because knowledge wasn’t just power anymore.  
    \textbf{Knowledge was risk.}

    \medskip
    
    If an executive knew something was broken, misreported, or fraudulent—and signed the certification anyway—they weren’t just risking bad press.  
    They were risking prison.

    \medskip
    
    And so a new, quieter defense strategy took hold in boardrooms and corner offices across America:
    
    \begin{quote}
    “Whatever you do... don’t tell me.”
    \end{quote}
    
    Whistleblowers became legal liabilities.  
    Audit findings were radioactive.  
    The less an executive knew, the less they could be accused of concealing.

    \medskip
    
    Ironically, SOX’s attempt to increase accountability also fostered a culture of deliberate ignorance—where the highest-paid people in the room had a vested interest in staying out of the loop.
    
    \medskip
    
    \textit{Which explains something engineers have been complaining about for decades:} 
    When they say “leadership is clueless,” they’re absolutely right.

    \medskip
    
    What they don’t know is...  \textbf{it’s by design.}
    
\end{HistoricalSidebar}

    \medskip
    
    And so the team scrambled.
    
    They formed a ``model remediation task force''... which was really just the same engineers, in the same Slack channel, but with a new calendar invite.
    
    Every meeting began with the same three questions:
    
    \begin{enumerate}
        \item “What exactly broke?”
        \item “Why didn’t we know it would break?”
        \item “Who’s writing the slides for the board update?”
    \end{enumerate}
    
    At first, the intern was pulled into meetings as a formality.
    
    “Talk to the intern,” someone said in chat.
    
    “Ha, yeah — the intern’s the SME now,” someone else replied jokingly.

    \medskip

\begin{HistoricalSidebar}{“Subject Matter Expert”—A Title Bestowed, Not Earned}

In theory, a \textbf{Subject Matter Expert (SME)} is someone with deep, specialized knowledge in a particular domain.  

\medskip

In practice, an SME is whoever’s name gets typed into the chat when a question goes unanswered.

\medskip

The term emerged alongside the rise of formalized knowledge management in the late 20th century, as corporations sought to map expertise onto org charts. But somewhere along the way, the process inverted:  

\medskip

Expertise wasn’t something you proved; It was something you were \textit{labeled}.

\begin{quote}
“The intern? Yeah, they’re the SME now.”
\end{quote}

And so were born titles like \textit{Cloud Strategist}, \textit{Cloud-Native Strategist}, \textit{Hybrid-Cloud Engineer}, \textit{Multi-Cloud Engineer}, \textit{Innovation Visionary}, and \textit{AI Transformation Lead}: roles defined less by measurable skill, and more by the need to write something on a conference badge.

\medskip

In corporate lore, being an SME isn’t about credentials or mastery.  It’s about being the last person standing when the spotlight turns.  

\begin{quote}
Congratulations. You’re the expert now.
\end{quote}

\end{HistoricalSidebar}

    \medskip
    
    But then it stuck.
    
    And once it stuck, it metastasized.
    
    He wasn’t just an intern anymore.
    
    He was the “subject matter expert.”
    
    \textit{Officially.}
    
    The title was written into the remediation report.  
    The title was printed on the org chart.  
    The title was spoken aloud at the compliance review.
    
    And once compliance had a name—his name—they didn’t care who the actual lead engineer was.
    
    Every question flowed to him.
    
    Every email CC’d him.
    
    Every Slack mention @’ed him.
    
    “Can you confirm the regression threshold was compliant with SOC-2 subsection 4.3.1(b)?”  
    “...What’s SOC-2 subsection 4.3.1(b)?”
    
    “Did we validate drift against production telemetry?”  
    “...What’s telemetry?”
    
    “Do you have an updated data lineage diagram?”  
    “...I ...I have a CSV?”
    
    At first, he answered from memory.
    
    Then he realized memory wasn’t enough.
    
    Compliance wasn’t going away.
    
    \textbf{They wanted real answers.}
    
    So he had to find them.
    
    He spent the next three weeks talking to every team that might be upstream.
    
    Analytics.  
    Observability.  
    Site Reliability.  
    Data Engineering.  
    An ancient team known only as “Telemetry Ops,” reachable only via an unmaintained group email.
    
    He read dashboards no one had logged into for years.  
    He traced cronjobs that hadn’t been touched since the last reorg.  
    He discovered the telemetry logs weren’t coming from a single system, but from six different services stitched together with sidecar scripts and duct tape.
    
    And little by little, he learned.
    
    Not because he wanted to.
    
    Not because it was his job.
    
    But because once compliance wrote his name down,  
    he had no choice.
    
    The intern wasn’t the scapegoat anymore.
    
    He was the face.
    
    He wasn’t just “responsible.”
    
    He was accountable.
    
    And by the time the audit concluded, the outcome was inevitable:
    
    \begin{quote}
    They didn’t hire him because he was brilliant.
    
    They hired him because—  
    by accident, by inertia, by the quiet violence of institutional entropy—  
    he had become the last living person who understood how the telemetry fed into the compliance pipeline.
    \end{quote}
    
    The model wasn’t just a model anymore.
    
    It had been referenced in quarterly filings.  
    It had been cited in internal audit trails.  
    It had been exported, as documentation, to an official government oversight body.
    
    And because no one else dared touch it—  
    because no one else knew how the system fit together—  
    the intern wasn’t just an engineer.
    
    He was infrastructure.
    
    And so he stayed.
    
    A little older.
    
    A little quieter.
    
    A little more haunted.
    
    He learned to wear headphones in meetings.  
    He learned to keep his calendar blocked with “focus time.”  
    He learned never to sit near Legal at the all-hands.
    
    The box was checked.
    
    The dashboard was green.
    
    But somewhere, deep in the telemetry logs,  
    another flag—  
    a new flag—  
    was quietly being written.
    
    No one knew what it meant.
    
    No one wanted to ask.
    
    And only the SME knew how to fix it.

    




    \subsection{A Case Study in Systemantics: Law 2 and Law 4 in Action}

    The story of the intern wasn’t just a tale of bad luck or oversight. It was a textbook demonstration of two key principles from John Gall’s \textit{Systemantics}:
    
    \begin{enumerate}
        \item \textbf{Law 2: “A complex system that works is invariably found to have evolved from a simple system that worked.”}
        \item \textbf{Law 4: “The real-world functioning of a system is never what its designer intended.”}
    \end{enumerate}
    
    The compliance model was never designed to be a compliance system.  
    It was a checkbox.  
    A stopgap.  
    A throwaway script meant to satisfy an auditor’s brief glance.
    
    But even a throwaway script is still a system.  
    And like all systems, it evolved.
    
    \medskip
    
    The moment it was deployed into production, the model became embedded inside operational workflows.  
    Its outputs fed dashboards.  
    Those dashboards became compliance evidence.  
    Those compliance dashboards became quarterly filings.
    
    \begin{quote}
    What began as a simple “yes, we have anomaly detection” turned into a fragile dependency no one could replace without rebuilding the whole thing.
    \end{quote}
    
    The system didn’t evolve by design.  
    It evolved by inertia.
    
    Each patch, each manual upload, each undocumented workaround added complexity without adding robustness.  
    Every layer buried the system deeper inside institutional processes.
    
    And so, by the time the audit arrived, it wasn’t a script anymore.  
    It was infrastructure.  
    It was compliance.  
    It was \textbf{the system}.
    
    \medskip
    
    \textbf{And that’s where Law 4 reared its head:}  
    The real-world functioning of the system had nothing to do with what its creators intended.
    
    No one intended for an intern’s hastily written script to become mission-critical.  
    No one intended for telemetry logs of unknown origin to feed compliance dashboards.  
    No one intended for SOC-2 controls to hinge on a CSV exported by hand every month.
    
    \begin{quote}
    But the system didn’t care what they intended.
    It only cared what they implemented.
    \end{quote}
    
    \medskip
    
    \textbf{The root failure wasn’t just technical—it was structural.}
    
    The team had centralized the entire compliance pipeline in a single monolith, operated by a single engineer, without modularity or separation of concerns.
    
    Had they used a more modular architecture—like an Airflow DAG or another orchestrated workflow—they could have decoupled the stages:
    
    \begin{itemize}
        \item Data ingestion
        \item Data preprocessing
        \item Model training
        \item Model validation
        \item Model deployment
        \item Compliance reporting
    \end{itemize}
    
    Each stage could have been owned by a different team, reviewed by domain experts, version-controlled independently, and validated against changing requirements.
    
    Instead, every function was locked inside a single script, a single person, a single point of failure.
    
    \medskip
    
    \textbf{And so, when it failed—it failed all at once.}
    
    The system wasn’t modular.  
    It wasn’t transparent.  
    It wasn’t adaptable.
    
    It was brittle.  
    And when brittle systems break, they don’t degrade gracefully.  
    They shatter.
    
    \medskip
    
    \begin{quote}
    The real tragedy?  
    They didn’t inherit a complex system.
    They built one—from a simple one—by accident.
    \end{quote}
    
    And because they never re-architected it, they guaranteed that complexity would ossify rather than evolve.
    
    A lesson written not just in Systemantics, but now, in their own compliance audit.
    














\subsection{Act V: The Pipeline That Could Have Been}

In a better world—one with deadlines that flex and teams that plan—this story could’ve gone very differently.

Instead of a one-shot pickle file hurled into prod like a cursed artifact, they could have built a proper pipeline.

Not a monolith. Not a notebook duct-taped into a cron job. A real, honest-to-goodness data pipeline. One with stages. One with structure. One that didn’t cause PagerDuty to cry in the middle of a sprint demo.

\textbf{Enter: Airflow.}

With Airflow, each step of the intern’s unholy monolith could’ve been a task in a Directed Acyclic Graph (DAG). Each task could’ve had retries. Timeouts. Logging. Alerts that didn’t rely on Slack sleuthing.

\begin{itemize}
  \item A task for loading logs (with schema checks, no less!).
  \item A task for parsing and validating those logs (with actual error handling!).
  \item A task for cleaning and transforming the data (instead of blindly \texttt{dropna()}-ing your sins away).
  \item A task for training the model (with versioning, reproducibility, and metrics).
  \item And a final task for deployment (ideally, not with \texttt{scp}).
\end{itemize}

Each step modular. Each failure traceable. Each retry logged, monitored, and contained.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
      node distance=1.1cm and 1.8cm,
      dagnode/.style={
        draw, rounded corners, minimum width=2.4cm, minimum height=0.8cm,
        font=\scriptsize, align=center, fill=blue!5
      },
      errnode/.style={
        draw, dashed, minimum width=2.4cm, minimum height=0.8cm,
        font=\scriptsize, align=center, fill=red!10
      },
      groupbox/.style={
        draw, rounded corners, thick, inner sep=0.5em, fill=blue!2!white
      },
      arrow/.style={->, thick}
    ]
    
    % Main flow nodes
    \node[dagnode, fill=gray!10] (start) {Start};
    \node[dagnode, below=of start] (load) {Load Logs};
    \node[dagnode, below=of load] (validate) {Validate Schema};
    \node[dagnode, below=of validate] (parse) {Parse Logs};
    \node[dagnode, below=of parse] (clean) {Clean Data};
    \node[dagnode, below=of clean] (train) {Train Model};
    \node[dagnode, below=of train] (evaluate) {Evaluate};
    \node[dagnode, below=of evaluate] (deploy) {Deploy Model};
    \node[dagnode, fill=gray!10, below=of deploy] (end) {End};
    
    % Error handler node
    \node[errnode, right=4.5cm of clean] (error) {Handle Error\\ (TriggerRule.ONE\_FAILED)};
    
    % Group box in the background layer
    \begin{pgfonlayer}{background}
      \node[groupbox, fit=(load)(validate)(parse)(clean)(train)(evaluate)(deploy), label={[align=center]above:\textbf{Execution Block}}] (taskgroup) {};
    \end{pgfonlayer}
    
    % Main path arrows
    \draw[arrow] (start) -- (load);
    \draw[arrow] (load) -- (validate);
    \draw[arrow] (validate) -- (parse);
    \draw[arrow] (parse) -- (clean);
    \draw[arrow] (clean) -- (train);
    \draw[arrow] (train) -- (evaluate);
    \draw[arrow] (evaluate) -- (deploy);
    \draw[arrow] (deploy) -- (end);
    
    % Shared error handler arrow
    \draw[arrow, dashed] (taskgroup.east) -- (error.west);
    
    \end{tikzpicture}
    \caption{Airflow DAG with a linear execution block and a shared error handler.}
    \label{fig:airflowdag_grouped}
\end{figure}
    
    
    


Instead of a haunted Roomba running in silence, it could’ve been a well-lit assembly line with guardrails, dashboards, and dignity.

Better yet, Airflow could’ve scheduled the job, rather than depending on a Bash script tied to someone’s user crontab. If the logs changed? The schema check would’ve failed. The DAG would’ve paused. Someone would’ve been notified before the VP’s dashboard went full red alert.

And most importantly—every DAG run would’ve left breadcrumbs: metadata, timestamps, artifact hashes. Something future interns could follow without needing a séance.

\begin{itemize}
    \item \textbf{Functions?} Written once, reused across tasks.  
    \item \textbf{Secrets?} Stored in a vault, not hardcoded in a script named \texttt{lolmodel.py}.  
    \item \textbf{Logs?} Structured, queryable, archived.  
    \item \textbf{Documentation?} Okay, maybe that’s still wishful thinking. But everything else? Achievable.
\end{itemize}

Sure, it might’ve taken an extra week to set up.  Sure, it wouldn’t fit in a single slide.  But it would’ve survived the reorg.  It would’ve survived Friday.

And no one would’ve had to grep for salvation.


\begin{lstlisting}[caption={Load logs from a directory.}, label={lst:load_logs}]
    def load_logs(log_dir: str) -> List[str]:
        import os
        log_lines = []
        for filename in os.listdir(log_dir):
            if filename.endswith(".log"):
                with open(os.path.join(log_dir, filename), "r") as f:
                    log_lines.extend(f.readlines())
        if not log_lines:
            raise FileNotFoundError("No logs found in directory.")
        return log_lines
\end{lstlisting}
    

\begin{lstlisting}[caption={Parse structured fields from log lines.}, label={lst:parse_logs}]
    def parse_logs(lines: List[str]) -> List[Tuple[float, float, float]]:
        import re
        parsed = []
        for line in lines:
            match = re.search(r"feature1=(\d+\.\d+).*?(feature2|feature3)=(\d+\.\d+).*?target=(\d+\.\d+)", line)
            if match:
                f1 = float(match.group(1))
                fx = float(match.group(3))
                target = float(match.group(4))
                parsed.append((f1, fx, target))
            else:
                print(f"[WARN] Failed to parse line: {line.strip()}")
        return parsed
\end{lstlisting}



\begin{lstlisting}[caption={Clean and standardize the parsed DataFrame.}, label={lst:clean_data}]
    def clean_data(df: pd.DataFrame) -> pd.DataFrame:
        df = df.rename(columns={"feature2": "featureX", "feature3": "featureX"})
        df = df.dropna().reset_index(drop=True)
        if df.empty:
            raise ValueError("No clean data left after dropping nulls.")
        return df
\end{lstlisting}



\begin{lstlisting}[caption={Train a linear model using scikit-learn.}, label={lst:train_model}]
    def train_model(df: pd.DataFrame) -> LinearRegression:
        from sklearn.linear_model import LinearRegression
        from sklearn.model_selection import train_test_split
    
        X = df[["feature1", "featureX"]]
        y = df["target"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
        model = LinearRegression()
        model.fit(X_train, y_train)
        return model
\end{lstlisting}


\begin{lstlisting}[caption={Deploy the trained model by pickling it.}, label={lst:deploy_model}]
    def deploy_model(model, output_path="final_model_v4.pkl"):
        import pickle
        with open(output_path, "wb") as f:
            pickle.dump(model, f)
        print(f"[INFO] Model saved to {output_path}")
        # Simulate deployment
        # subprocess.run(["scp", output_path, "prod-server:/var/www/html/models/"])
\end{lstlisting}


\begin{lstlisting}[caption={Airflow DAG with linear tasks and error handling flow.}, label={lst:airflowdag_error_handling}]
    from airflow.decorators import dag, task
    from airflow.operators.empty import EmptyOperator
    from airflow.operators.python import PythonOperator
    from airflow.utils.trigger_rule import TriggerRule
    from datetime import datetime
    
    @dag(schedule_interval="@daily", start_date=datetime(2023, 1, 1), catchup=False, tags=["modular_pipeline"])
    def resilient_model_pipeline():
    
        start = EmptyOperator(task_id="start")
        end = EmptyOperator(task_id="end")
    
        # Fallback error handler task
        error_handler = PythonOperator(
            task_id="handle_error",
            python_callable=lambda: print("[ERROR] Pipeline failure detected. Escalating."),
            trigger_rule=TriggerRule.ONE_FAILED
        )
    
        @task()
        def load_logs_task():
            return load_logs("/mnt/logs")
    
        @task()
        def validate_schema_task(logs):
            if not logs:
                raise ValueError("Log validation failed: Empty or missing.")
            return logs
    
        @task()
        def parse_logs_task(validated_logs):
            return parse_logs(validated_logs)
    
        @task()
        def clean_data_task(parsed):
            import pandas as pd
            df = pd.DataFrame(parsed, columns=["feature1", "featureX", "target"])
            return clean_data(df)
    
        @task()
        def train_model_task(df):
            return train_model(df)
    
        @task()
        def evaluate_model(model):
            print("[INFO] Evaluating model...")
            return "evaluation complete"
    
        @task()
        def deploy_model_task(eval_result, model):
            deploy_model(model)
    
        # DAG wiring
        logs = load_logs_task()
        validated = validate_schema_task(logs)
        parsed = parse_logs_task(validated)
        cleaned = clean_data_task(parsed)
        model = train_model_task(cleaned)
        eval_result = evaluate_model(model)
        deploy = deploy_model_task(eval_result, model)
    
        # Dependencies
        start >> logs >> validated >> parsed >> cleaned >> model >> eval_result >> deploy >> end
    
        # Attach error handler to all main tasks (except start/end)
        for t in [logs, validated, parsed, cleaned, model, eval_result, deploy]:
            t >> error_handler
    
    dag = resilient_model_pipeline()
    \end{lstlisting}





\subsection{version 2}



\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
      node distance=1.1cm and 1.8cm,
      dagnode/.style={
        draw, rounded corners, minimum width=2.6cm, minimum height=0.8cm,
        font=\scriptsize, align=center, fill=blue!5
      },
      errnode/.style={
        draw, dashed, minimum width=2.6cm, minimum height=0.8cm,
        font=\scriptsize, align=center, fill=red!10
      },
      groupbox/.style={
        draw, rounded corners, thick, inner sep=0.5em, fill=blue!2!white
      },
      arrow/.style={->, thick}
    ]
    
    % Main flow
    \node[dagnode, fill=gray!10] (start) {Start};
    \node[dagnode, below=of start] (load) {Load Logs};
    \node[dagnode, below=of load] (validate) {Validate Schema};
    \node[dagnode, below=of validate] (parse) {Parse Logs};
    
    % Parallel tasks
    \node[dagnode, below left=1.2cm and 1.5cm of parse] (quality) {Validate Quality};
    \node[dagnode, below right=1.2cm and 1.5cm of parse] (features) {Feature Engineering};
    
    % Join and continue
    \node[dagnode, below=1.8cm of parse] (train) {Train Model};
    \node[dagnode, below=of train] (evaluate) {Evaluate};
    \node[dagnode, below=of evaluate] (deploy) {Deploy Model};
    \node[dagnode, fill=gray!10, below=of deploy] (end) {End};
    
    % Error handler
    \node[errnode, right=5.3cm of train] (error) {Handle Error\\ (TriggerRule.ONE\_FAILED)};
    
    % Group box in background
    \begin{pgfonlayer}{background}
      \node[groupbox, fit=(load)(validate)(parse)(quality)(features)(train)(evaluate)(deploy), 
            label={[align=center]above:\textbf{Execution Block}}] (taskgroup) {};
    \end{pgfonlayer}
    
    % Main arrows
    \draw[arrow] (start) -- (load);
    \draw[arrow] (load) -- (validate);
    \draw[arrow] (validate) -- (parse);
    \draw[arrow] (parse) -- (quality);
    \draw[arrow] (parse) -- (features);
    \draw[arrow] (quality) -- (train);
    \draw[arrow] (features) -- (train);
    \draw[arrow] (train) -- (evaluate);
    \draw[arrow] (evaluate) -- (deploy);
    \draw[arrow] (deploy) -- (end);
    
    % Error path from group
    \draw[arrow, dashed] (taskgroup.east) -- (error.west);
    
    \end{tikzpicture}
    \caption{Airflow DAG with parallel validation and feature engineering inside the execution block, and a shared error handler.}
    \label{fig:airflowdag_parallel}
\end{figure}




\begin{lstlisting}[caption={Airflow DAG with parallel data quality validation and feature engineering before training.}, label={lst:airflowdag_parallel}]
    from airflow.decorators import dag, task
    from airflow.operators.empty import EmptyOperator
    from airflow.operators.python import PythonOperator
    from airflow.utils.trigger_rule import TriggerRule
    from datetime import datetime
    
    @dag(schedule_interval="@daily", start_date=datetime(2023, 1, 1), catchup=False, tags=["modular_pipeline"])
    def enhanced_pipeline():
    
        start = EmptyOperator(task_id="start")
        end = EmptyOperator(task_id="end")
    
        # Shared error handler
        error_handler = PythonOperator(
            task_id="handle_error",
            python_callable=lambda: print("[ERROR] Pipeline failure detected."),
            trigger_rule=TriggerRule.ONE_FAILED
        )
    
        @task()
        def load_logs_task():
            return load_logs("/mnt/logs")
    
        @task()
        def validate_schema_task(logs):
            if not logs:
                raise ValueError("Log validation failed: Empty or missing.")
            return logs
    
        @task()
        def parse_logs_task(validated_logs):
            return parse_logs(validated_logs)
    
        @task()
        def validate_quality_task(parsed_data):
            # e.g., check missing values, outliers, etc.
            print("Validating data quality...")
            return parsed_data
    
        @task()
        def generate_features_task(parsed_data):
            # e.g., feature transformation, normalization
            print("Engineering features...")
            return parsed_data
    
        @task()
        def train_model_task(inputs1, inputs2):
            import pandas as pd
            # Merge cleaned inputs
            df = pd.DataFrame(inputs1, columns=["feature1", "featureX", "target"])
            return train_model(df)
    
        @task()
        def evaluate_model(model):
            print("[INFO] Evaluating model...")
            return "evaluation complete"
    
        @task()
        def deploy_model_task(eval_result, model):
            deploy_model(model)
    
        # Wiring
        logs = load_logs_task()
        validated = validate_schema_task(logs)
        parsed = parse_logs_task(validated)
    
        quality = validate_quality_task(parsed)
        features = generate_features_task(parsed)
    
        model = train_model_task(quality, features)
        eval_result = evaluate_model(model)
        deploy = deploy_model_task(eval_result, model)
    
        start >> logs >> validated >> parsed
        parsed >> [quality, features]
        [quality, features] >> model >> eval_result >> deploy >> end
    
        for t in [logs, validated, parsed, quality, features, model, eval_result, deploy]:
            t >> error_handler
    
    dag = enhanced_pipeline()
\end{lstlisting}

    


\subsection{version 3}


\begin{lstlisting}[caption={Airflow DAG with parallel data quality validation and feature engineering before training.}, label={lst:airflowdag_parallel}]
from airflow.decorators import dag
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.utils.trigger_rule import TriggerRule
from datetime import datetime

from tasks.load_logs import load_logs_task
from tasks.validate_schema import validate_schema_task
from tasks.parse_logs import parse_logs_task
from tasks.validate_quality import validate_quality_task
from tasks.generate_features import generate_features_task
from tasks.train_model import train_model_task
from tasks.evaluate_model import evaluate_model_task
from tasks.deploy_model import deploy_model_task

@dag(schedule_interval="@daily", start_date=datetime(2023, 1, 1), catchup=False, tags=["modular_pipeline"])
def enhanced_pipeline():
    start = EmptyOperator(task_id="start")
    end = EmptyOperator(task_id="end")

    error_handler = PythonOperator(
        task_id="handle_error",
        python_callable=lambda: print("[ERROR] Pipeline failure detected."),
        trigger_rule=TriggerRule.ONE_FAILED
    )

    logs = load_logs_task()
    validated = validate_schema_task(logs)
    parsed = parse_logs_task(validated)

    quality = validate_quality_task(parsed)
    features = generate_features_task(parsed)

    model = train_model_task(quality, features)
    eval_result = evaluate_model_task(model)
    deploy = deploy_model_task(eval_result, model)

    start >> logs >> validated >> parsed
    parsed >> [quality, features]
    [quality, features] >> model >> eval_result >> deploy >> end

    for t in [logs, validated, parsed, quality, features, model, eval_result, deploy]:
        t >> error_handler

dag = enhanced_pipeline()
\end{lstlisting}




\lstdefinestyle{tree}{
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{gray!5},
  frame=single,
  columns=fullflexible,
  showstringspaces=false
}

\begin{lstlisting}[style=tree, caption={Modular DAG file structure (ASCII-safe)}, label={lst:dag_tree_ascii}]
    dags/
    |- enhanced_pipeline_dag.py         # DAG definition & task wiring
    |- tasks/
       |- __init__.py
       |- load_logs.py
       |- validate_schema.py
       |- parse_logs.py
       |- validate_quality.py
       |- generate_features.py
       |- train_model.py
       |- evaluate_model.py
       |- deploy_model.py
\end{lstlisting}



\subsection{version 4}

\begin{lstlisting}[style=tree, caption={Modular DAG file structure (ASCII-safe)}, label={lst:dag_tree_ascii}]
    dags/
    |- enhanced_pipeline/
    |  |- __init__.py
    |  |- enhanced_pipeline_dag.py         # DAG definition + @dag wrapper
    |  |- task_factory.py                  # contains create_tasks()
    |  |- task_wiring.py                   # contains wire_tasks()
    |  |- orchestrator.py                  # contains orchestrate_dag()
    |
    |- tasks/
    |  |- __init__.py
    |  |- load_logs.py
    |  |- validate_schema.py
    |  |- parse_logs.py
    |  |- validate_quality.py
    |  |- generate_features.py
    |  |- train_model.py
    |  |- evaluate_model.py
    |  |- deploy_model.py
    |
    |- utils/
       |- __init__.py
       |- io.py                            # e.g., load_logs()
       |- ml.py                            # e.g., train_model(), etc.
    
\end{lstlisting}

\begin{lstlisting}[caption={Airflow DAG with parallel data quality validation and feature engineering before training.}, label={lst:airflowdag_parallel}]
    from airflow.decorators import dag
    from airflow.operators.empty import EmptyOperator
    from airflow.operators.python import PythonOperator
    from airflow.utils.trigger_rule import TriggerRule
    from datetime import datetime
    
    from tasks.load_logs import load_logs_task
    from tasks.validate_schema import validate_schema_task
    from tasks.parse_logs import parse_logs_task
    from tasks.validate_quality import validate_quality_task
    from tasks.generate_features import generate_features_task
    from tasks.train_model import train_model_task
    from tasks.evaluate_model import evaluate_model_task
    from tasks.deploy_model import deploy_model_task
    
    
    def create_tasks():
        """Return a dictionary of all tasks with dependencies left unwired."""
        start = EmptyOperator(task_id="start")
        end = EmptyOperator(task_id="end")
    
        error_handler = PythonOperator(
            task_id="handle_error",
            python_callable=lambda: print("[ERROR] Pipeline failure detected."),
            trigger_rule=TriggerRule.ONE_FAILED
        )
    
        logs = load_logs_task()
        validated = validate_schema_task(logs)
        parsed = parse_logs_task(validated)
    
        quality = validate_quality_task(parsed)
        features = generate_features_task(parsed)
    
        model = train_model_task(quality, features)
        eval_result = evaluate_model_task(model)
        deploy = deploy_model_task(eval_result, model)
    
        return {
            "start": start,
            "end": end,
            "error_handler": error_handler,
            "logs": logs,
            "validated": validated,
            "parsed": parsed,
            "quality": quality,
            "features": features,
            "model": model,
            "eval_result": eval_result,
            "deploy": deploy
        }
    
    
    def wire_tasks(t):
        """Connect tasks in the desired DAG structure."""
        t["start"] >> t["logs"] >> t["validated"] >> t["parsed"]
        t["parsed"] >> [t["quality"], t["features"]]
        [t["quality"], t["features"]] >> t["model"] >> t["eval_result"] >> t["deploy"] >> t["end"]
    
        for task in [
            t["logs"], t["validated"], t["parsed"],
            t["quality"], t["features"],
            t["model"], t["eval_result"], t["deploy"]
        ]:
            task >> t["error_handler"]
    
    
    def orchestrate_dag():
        """High-level orchestration of task creation and wiring."""
        tasks = create_tasks()
        wire_tasks(tasks)
    
    
    @dag(schedule_interval="@daily", start_date=datetime(2023, 1, 1), catchup=False, tags=["modular_pipeline"])
    def enhanced_pipeline():
        orchestrate_dag()
    
    
dag = enhanced_pipeline()
\end{lstlisting}
    
\begin{table}[H]
    \centering
    \begin{tabular}{|l|p{10cm}|}
    \hline
    \textbf{File} & \textbf{Responsibility} \\
    \hline
    \texttt{enhanced\_pipeline\_dag.py} & Contains \texttt{@dag} decorator and calls \texttt{orchestrate\_dag()} \\
    \hline
    \texttt{task\_factory.py} & Defines and returns all task objects (\texttt{create\_tasks()}) \\
    \hline
    \texttt{task\_wiring.py} & Wires the task dependencies (\texttt{wire\_tasks(tasks)}) \\
    \hline
    \texttt{orchestrator.py} & Calls both functions (\texttt{orchestrate\_dag()}) \\
    \hline
    \texttt{tasks/*.py} & Each contains a single \texttt{@task} Airflow function \\
    \hline
    \texttt{utils/io.py} & Low-level data access helpers (e.g., log loading) \\
    \hline
    \texttt{utils/ml.py} & Model logic, feature engineering, and evaluation helpers \\
    \hline
    \end{tabular}
    \caption{Modular DAG code responsibilities by file}
    \label{tab:dag_structure}
\end{table}
    

    





\subsection{Epilogue: Why Modularity Matters}

Let’s be honest.

This story didn’t end in disaster because the intern was reckless.  It ended in disaster because the system had no guardrails.

When a one-off training script, duct-taped together with assumptions, globals, and good intentions, is all it takes to deploy to production, the issue isn’t code quality---it’s architecture.

Modularity isn’t just a buzzword consultants put in slide decks.  It’s a survival strategy.

If the ingestion logic had been its own component---versioned, tested, isolated---a format change wouldn’t have brought down the system.\\
If training and deployment had been separate stages with inputs, outputs, and contracts, we wouldn’t have been staring at a mystery \texttt{.pkl} file like it was an alien artifact.

Modularity gives you reproducibility.  It gives you traceability. It lets you swap a broken piece without disassembling the entire machine.

Without it?

You get fragile scripts, ghost models, and institutional knowledge stored in a single intern’s memory.

Which is fine\ldots until they graduate.


\vspace{1em}
\textbf{A Visual, in Your Mind}

Picture a Jenga tower.

A modular system lets you pull out one block and put in a better one.\\
A monolithic system? One wrong move, and the whole thing comes crashing down.

Your move.

\subsection{Case Study Revisited: The Intern and the Forgotten Monolith}

Imagine if this methodology had been applied before the intern ever touched \texttt{pandas.read\_csv()}.

A mindful diagnostic session might have revealed:
\begin{itemize}
    \item The system was already exhibiting **Shadow** behaviors (ancient scripts no one understood).
    \item The ownership was ambiguous, signaling a Conway-induced **Orphan**.
    \item The culture prioritized speed over structure—a breeding ground for Hero-driven quick fixes.
\end{itemize}

Instead of celebrating the intern’s short-term “success,” the team could have paused, recognized these patterns, and intervened—not with a heavy governance hand, but with lightweight guardrails:

\begin{itemize}
    \item Mandate modularity not as a rule, but as a reflective practice.
    \item Document the why, not just the what.
    \item Treat every ad-hoc script as a potential **patient zero** for future dysfunction.
\end{itemize}






