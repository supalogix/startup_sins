\section{Modularity: Because the Intern Deployed a Hardcoded Model to Prod}

\begin{figure}[H]
\centering

% === First row ===
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Engineer 1}
    {Engineer 2}
    {\footnotesize Let's just log it like this for now. I'll document it later.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{Prologue: It begins, as always, with good intentions.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Engineer 1}
    {Engineer 2}
    {\footnotesize v2FlagTemp? Yeah, just set it to TRUE if the system feels unstable.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{The design philosophy: chaos with confidence.}
\end{subfigure}

\vspace{1em}

% === Second row ===
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Executive 1}
    {Executive 2}
    {\footnotesize We’re preparing a strategic reorg. Most of the original team will be gone by Friday.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{The culling: efficiencies must be unlocked.}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.45\textwidth}
\centering
\begin{tikzpicture}
  \comicpanel{0}{0}
    {Executive 1}
    {Executive 2}
    {\footnotesize But I’m not worried. I’ve been assured everything is fully documented.}
    {(0,-0.6)}
\end{tikzpicture}
\caption*{The punchline: spoken like someone who’s never used Confluence.}
\end{subfigure}

\caption*{The Birth of a Legacy System: High hopes and zero documentation.}
\end{figure}

\subsection{Act I: The Baptism}

It always starts the same way.

An intern joins a big company: the kind with seventeen internal platforms named after birds, seven ways to deploy code, and exactly zero onboarding documentation that's less than two fiscal years out of date.

A company so deep in perpetual reorgs that entire teams don’t even know which floor they're on anymore—let alone where the bathrooms are. Asking for directions gets you a Slack thread, a deprecated wiki link, and a shrug.

\medskip

\begin{HistoricalSidebar}{The MBA Mindset—When Management Became a Spreadsheet}
    Business schools promised to turn managers into leaders.  

    \medskip

    What they really did was turn leaders into spreadsheet jockeys.

    \medskip
    
    Starting in the postwar boom of the 1950s, \textbf{management theory became an academic discipline}, a way to turn messy human organizations into controllable systems. First it was \textit{Taylorism} and \textit{scientific management}, slicing every task into a stopwatch-measured unit. 

    \medskip
    
    Then came \textit{Six Sigma}, \textit{Lean}, \textit{Lean Six Sigma}, \textit{Agile}, \textit{Scaled Agile Framework}, and \textit{Disciplined Agile Delivery} --- each one a new operating system for the corporation, each one a promise of smoother flow, fewer defects, higher ROI.

    \medskip
    
    But underneath the jargon lay a simple principle:  

    \begin{quote}
    \textbf{Everything is a process. Everything can be optimized. Everyone is a cell in the workbook.}
    \end{quote}
    
    To the MBA mind, the org chart isn’t a map of people...  it’s a pivot table.  

    \medskip
    
    A “vertical reporting structure” isn’t about hierarchy... it’s about columns vs. rows.  

    \medskip

    A “horizontal reorg” isn’t a cultural shift... it’s a filter operation in Excel.  

    \medskip

    And when you get “consolidated,” it’s just another \texttt{SUM()} function rolling up a few line items.

    \medskip
    
    In this worldview, the company isn’t a living organism.  It’s a spreadsheet looking for a macro.
    
    \begin{quote}
    \textit{Management isn’t about leading people. It’s about formatting cells.}
    \end{quote}
\end{HistoricalSidebar}

\medskip

Their mission? “Help out with a compliance thing.”

Here’s what happened: the Chief Compliance Officer had swung by engineering earlier that week holding a printout from some internal audit checklist. “We’re supposed to have ‘automated anomaly detection’ on user session logs for SOC-2 subsection 4.3.1 compliance,” she’d said, reading the words like they were spells in an unfamiliar language. “We don’t actually need alerts or dashboards or anything. We just need to show we *have* something.”

The lead engineer had nodded solemnly, understanding immediately: this wasn’t about detecting anomalies. This was about **checking a box** so the auditors would go away.

And he had just the solution.

“Let’s give it to the intern,” he said.

After all, it wasn’t a revenue feature. It wasn’t security-critical. It wasn’t even a real model request—it was compliance theater: a binary classifier whose mere existence was enough to satisfy a requirement nobody cared to interpret too closely.

The rule itself—SOC-2 subsection 4.3.1—was a vague line item about “proactive identification of anomalous user activity.” Not fraud detection. Not real-time monitoring. Just some plausible process that could be waved at an auditor and described with buzzwords like “machine learning” and “automation.”

The model didn’t need to work. It didn’t need to catch anomalies. It just needed to **exist on paper**.

And yet — paradoxically — this model had to be robust, fast, explainable, and deployable by Friday. It’s the Schrödinger’s cat of machine learning: simultaneously fake and real, invisible and on fire.

Now here’s the catch: no one could tell them where the data came from. Or where it went. Or what shape it was in. Or even what system was writing to it.

All anyone knew was that somewhere—out there—something was emitting telemetry logs.

Not regularly. Not predictably. But reliably enough that every few hours, another log file appeared in a web dashboard no one remembered creating. The dashboard URL was buried inside an old wiki page under “temporary monitoring (delete later)”—last edited four years ago by an account that no longer existed.

\textbf{In other words: institutional memory had evaporated.}

\begin{HistoricalSidebar}{The Lost Knowledge of Apollo}

    During the Apollo era, NASA undertook the monumental task of landing humans on the Moon. This endeavor required the collaboration of thousands of engineers, scientists, and technicians, each contributing specialized knowledge to the mission's success.

    \medskip
    
    One notable figure was \textbf{Margaret Hamilton}, who led the software engineering team responsible for the Apollo Guidance Computer. Her team's work was crucial during the Apollo 11 mission, particularly when unexpected computer alarms threatened to abort the lunar landing. The robustness of their software allowed the mission to proceed safely.

    \medskip
    
    However, much of the intricate knowledge from the Apollo program was never comprehensively documented. As the program wound down and personnel moved on, NASA faced an uncomfortable realization: critical institutional memory was evaporating. Knowledge was dispersed across thousands of minds, each holding only a fragment of the whole. No single person knew how to put a human on the Moon.

    \medskip
    
    Efforts were made to capture what could be salvaged. Engineers interviewed veterans, scoured archives, compiled schematics. But some knowledge was already irretrievably lost. Processes that once depended on tacit understanding, undocumented workarounds, or informal networks of expertise had no paper trail to follow.

    \medskip
    
    It’s a cautionary tale: even a triumph like Apollo can become a legacy system. And once the last person leaves the room, you might find that no one knows how it actually worked.
    
\end{HistoricalSidebar}


So the intern did what anyone would do under the circumstances:



\begin{itemize}
    \item They clicked “Export CSV” from the mystery web dashboard.
    \item They opened the CSV in Jupyter like it was a sacred ritual passed down from Stack Overflow itself.
    \item They grep’ed.
\end{itemize}

And miraculously, it loaded… sort of.

There were nulls in half the columns, timestamps in three different formats, and a mysterious field called \texttt{v2FlagTemp} that no one ever defined.

Nobody knew what \texttt{v2FlagTemp} actually did. Some said it was a deprecation marker. Others claimed it toggled a legacy feature that hadn’t existed since the Kubernetes migration of ’19. One engineer swore it flipped to \texttt{TRUE} during solar flares.

Meanwhile, the timestamps were a small chaos engine unto themselves—sometimes ISO with a `T`, sometimes space-separated, sometimes slashed Y/M/D.

Parsing the logs felt less like data engineering and more like digital paleontology. You weren’t cleaning data. No, you were performing forensic analysis on a telemetry feed no one remembered enabling.

\begin{lstlisting}[
    caption={Sample telemetry logs exported from the mystery dashboard},
    label={lst:ancientlogs},
    basicstyle=\ttfamily\small,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    breaklines=true,
    breakatwhitespace=false,
    postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space}
  ]
2020-01-03T10:15:32Z level=INFO deviceId=node12 metricA=12.3 metricB=7.1 target=null v2FlagTemp=TRUE
2020-01-03 10:15:33 INFO deviceId=node12 metricA=13.1 metricB=8.0 target=4.0
2020/01/03 10:15:34 INFO deviceId=node12 metricA=nan metricB=6.9 target=null
2020-01-03 10:15:35 WARN deviceId=node17 v2FlagTemp=FALSE droppedPackets=5
2020-01-03T10:15:36Z level=INFO deviceId=node12 metricA=11.8 metricB=7.3 target=3.9 v2FlagTemp=1
2020-01-03T10:15:37Z level=ERROR deviceId=??? metricA=? metricB=? target=missing
2020-01-03 10:15:38 INFO deviceId=node12 metricA=13.0 metricB=8.1 target=4.1
\end{lstlisting}

But the intern pressed on because a few \texttt{dropna()} calls and some light string parsing would bring order to the chaos. Hope, after all, is the most dangerous optimizer.

\begin{HistoricalSidebar}{“Sort of Working” and the Robustness Principle}

    Software engineers have a term for this:  

    \begin{quote}
    “It sort of works.”
    \end{quote}
    
    Not flawless. Not elegant. But working—enough.

    \medskip
    
    This mindset traces back to an idea known as the \textbf{Robustness Principle}, coined by Jon Postel in 1981 during the early days of the Internet:  

    \begin{quote}
    \textit{“Be conservative in what you do, be liberal in what you accept from others.”}
    \end{quote}
    
    In essence: tolerate imperfect inputs, handle edge cases gracefully, and keep the system moving forward even when things don’t quite fit.  

    \medskip

    A philosophy of resilience over rigidity.  

    \medskip

    A quiet acknowledgment that in a messy, interconnected world, sometimes “sort of working” is good enough—because it keeps working.
    
    \medskip
    
    But here’s the twist: the same principle applies outside of software.

    \medskip
    
    In interpersonal relationships, it’s often said:  

    \begin{quote}
    \textit{“It’s better to be 50\% present 100\% of the time than 100\% present 50\% of the time.”}
    \end{quote}
    
    Why?  Because what people value most isn’t perfection.  It’s \textbf{reliability}.  

    \medskip

    If someone truly appreciates your company, they’d rather have half of you, consistently, than all of you, sporadically.

    \medskip
    
    The same is true of program features:  A clunky tool that’s always there beats a perfect tool that disappears when you need it most.
    
\end{HistoricalSidebar}

Armed with Stack Overflow tabs and unshakable faith in pandas, they began the sacred rite of modern data cleaning: (a) removing nulls without asking why they were there, (b) coercing all timestamps into ISO format (with silent fallback), and (c) mapping suspicious Boolean-like strings to something vaguely coherent. \texttt{v2FlagTemp}? Treated as a categorical, obviously. Unused features? Dropped with the righteous confidence of someone who had never seen a postmortem. \textbf{Because nothing says “production ready” like blindly transforming orphaned telemetry logs scraped from an unknown source.}

Fresh off the intoxicating high of successfully reading a CSV, the intern embarked on their next great odyssey.

They did what every unsupervised intern with vague requirements, no staging environment, and a deeply misplaced sense of destiny inevitably does: they wrote a script.

Not a pipeline. Not a collection of well-documented, reusable modules.

No. This was different.

It was a single, sprawling, glorious file --— an unbroken stream of code that took data from manual download to manual upload. From CSV import to model export to celebratory print statement. All in one script. All in one breath.

\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{teal},
  breaklines=true,
  breakatwhitespace=false,
  postbreak=\mbox{\textcolor{gray}{$\hookrightarrow$}\space},
  showstringspaces=false,
  frame=single,
  caption={The Monolith Script in All Its Glory},
  label={lst:monolith},
  numbers=left,
  numberstyle=\tiny,
  language=Python
}

\begin{lstlisting}[language=Python]
# the_monolith.py

import pandas as pd
import pickle
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Step 1: Load exported CSV manually downloaded from dashboard
csv_path = "/Users/intern/Downloads/mystery_telemetry_export.csv"
df = pd.read_csv(csv_path, sep=r'\s+', engine='python', error_bad_lines=False)

# Step 2: Parse columns using regex-like string splits
df['metricA'] = pd.to_numeric(df['metricA'], errors='coerce')
df['metricB'] = pd.to_numeric(df['metricB'], errors='coerce')
df['target'] = pd.to_numeric(df['target'], errors='coerce')

# Drop rows where target or features are missing
df = df.dropna(subset=['metricA', 'metricB', 'target']).reset_index(drop=True)

# Step 3: Train model
X = df[['metricA', 'metricB']]
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = LinearRegression()
model.fit(X_train, y_train)

# Step 4: Save model
with open("model.pkl", "wb") as f:
    pickle.dump(model, f)

# Step 5: Upload manually to compliance dashboard
print("Upload 'model.pkl' to compliance-portal.internal/upload and check the box manually.")

print("Model training complete. Nothing could possibly go wrong.")
\end{lstlisting}

It pulled packages from three different ecosystems, used regex-as-a-strategy, and had print statements doubling as deployment documentation.

There were no functions... only faith.  
There was no error handling... only optimism.

The monolith had been born.

\begin{HistoricalSidebar}{“Monolith”—From Stone to Software}

The word \textbf{monolith} traces back to the Ancient Greek \textit{monolithos}, from \textit{monos} (single) + \textit{lithos} (stone):  
\begin{quote}
\textit{“A single massive block of stone.”}
\end{quote}

Think Stonehenge. Think Easter Island. Think towering obelisks carved from one uninterrupted slab.  
A monument that is solid, unified, and immovable.

It wasn’t until the 20th century that engineers began borrowing the term for other domains: giant corporate structures, massive bureaucracies, or anything unwieldy and indivisible.

By the time it arrived in software engineering, \textbf{“monolith”} described applications built as a single, tightly coupled unit:  
No clean seams, no modular components, no easy way to break it apart.

In short:  
\begin{quote}
A structure as awe-inspiring—and as impossible to rearrange—as a stone temple.
\end{quote}

So when the script choked, it wasn’t just a failure.  
It was a monument to an architectural philosophy:  
\textbf{“Build once. Move never.”}
\end{HistoricalSidebar}


It was ugly. It was fragile. It was unreadable to everyone, including the intern who wrote it.

But—miraculously—it worked.

\begin{quote}
And for one brief, cursed moment, the system was in balance.
By the time the Jupyter notebook was done, the data looked “clean.”  
Not correct. Not meaningful. But clean enough to pass a code review;  
and that, in the intern’s defense, was all it needed to do to check the box.
\end{quote}




\subsection{Act II: The Fall}

Weeks passed.

No one touched the script. No one asked about the model.

It sat in its digital corner, whirring quietly inside a Docker container no one dared to rebuild, delivering predictions with the eerie consistency of a haunted Roomba—faithful, silent, and faintly menacing.

\begin{HistoricalSidebar}{“It Works on My Machine”—The Defense That Became Infrastructure}

For as long as software has existed, engineers have wielded a near-perfect excuse for bugs, delays, and failures:  

\begin{quote}
\textbf{It works on my machine.}
\end{quote}

A line so simple, so disarming, it carries just enough plausible deniability to stall accountability.  
How do you prove otherwise? Short of physically visiting the engineer’s desk or replicating every library, path, and environment variable—they win by default.

\medskip

It wasn’t just an excuse. It was a loophole.  
And loopholes scale.

\medskip

Like any unenforceable law, the rule of “software must work everywhere” slowly eroded in the face of practical impossibility. If you can’t enforce a law, the law doesn’t really exist.  
If everyone’s machine is different, then whose machine is “real”?

\medskip

\noindent Enter Docker.

\medskip

Docker didn’t eliminate the loophole. It industrialized it.  
Why fight “it works on my machine” when you can simply \textbf{ship the machine}?

\medskip

A container wasn’t just a runtime—it was a diplomatic treaty between development, testing, and production.  
A frozen snapshot of “my machine,” wrapped and sealed for transit.

\medskip

``It works on my machine'' had gone from punchline to product.  
And in doing so, it solved the problem in the most engineering way possible:  
not by closing the loophole, but by automating it.

\end{HistoricalSidebar}



Until, of course, the day it didn’t.

It was a Friday afternoon. The kind of Friday afternoon when half the team had already mentally clocked out and the other half was pretending to write documentation.

Then it happened.

\textbf{The logs changed.}

Not a dramatic change—just enough to be invisible to humans, but fatal to regex. A new column had quietly appeared, unnamed and smug. One old column, long relied upon, was gone without a trace. The date format had subtly shifted—no longer slashes, but dashes. Or was it the timezone? No one knew. The changes crept in like code gremlins, tiptoeing past CI pipelines.

And the script—poor, unsuspecting monolith that it was—choked.

The model broke.

The service failed.

Metrics flatlined. Dashboards turned a shade redder than anyone was comfortable with.

PagerDuty screamed into the void.

On-call phones vibrated on desks and nightstands with all the fury of a thousand missed deadlines. Slack channels lit up like a Christmas tree in a lightning storm. Messages began with \texttt{"Hey..."} and ended with \texttt{"@here."}

The intern, who had last touched the code three rotations ago, was summoned like a wizard in exile. Somewhere, their laptop opened slowly.

The autopsy hadn’t started, but the blame game had.

\medskip 

\begin{lstlisting}[caption={Slack transcript, 2:43 PM on a Friday}, label={lst:slackpanic}, basicstyle=\ttfamily\small, frame=single]
#sre-oncall

[2:43 PM] @alertbot: [ALERT] PROD Model Service Failure - HTTP 500s detected
[2:44 PM] @senior_sre: who owns this??
[2:44 PM] @eng_manager: wasnt this the interns thing?
[2:45 PM] @intern: hi yes uh give me 5 min
[2:46 PM] @senior_sre: what changed?
[2:46 PM] @intern: the logs have... evolved?
[2:47 PM] @intern: feature2 is now feature3
[2:47 PM] @intern: also the timestamps are in ISO format? I think?
[2:48 PM] @senior_sre: wheres the source?
[2:48 PM] @intern: it is not in Git
[2:48 PM] @senior_sre: what
[2:49 PM] @intern: I think I uploaded it through the command line from my VM last summer?
[2:49 PM] @eng_manager: do we have any docs?
[2:49 PM] @intern: there was a notebook... on my old laptop.
[2:50 PM] @alertbot: [ALERT] PROD Model Service Crash Loop Restarting
[2:51 PM] @senior_sre: this is why we dont YOLO deploy models
[2:51 PM] @intern: understood
[2:52 PM] @eng_manager: rewrite everything
\end{lstlisting}

They searched for the source code.

Desperation mounting, they combed through old repos, abandoned branches, shared drives with names like \texttt{archive\_final\_bkp\_DO\_NOT\_DELETE}. They unzipped tarballs nested within tarballs like digital matryoshka dolls. They even searched Confluence.

They found nothing.

No Git history. No documentation. Not even a rogue Jupyter notebook half-filled with markdown and regret.

They hadn’t deployed a model.  
They had released an unsupervised cryptid into production.

An eldritch pipeline, stitched together by interns long since graduated, silently shaping the fate of a product roadmap.

And now it was angry.

\begin{lstlisting}[caption={The log that broke the build}, label={lst:falllogs}, basicstyle=\ttfamily\small, frame=single]
2020-01-03 10:15:36 INFO  | feature1=13.0 feature2=8.1 target=4.1

% One week later...
2020-01-10 10:15:36 INFO  | feature1=13.0 feature3=foo status=OK timestamp=2020-01-10T10:15:36Z
\end{lstlisting}

\textbf{And that’s when it escalated.}

The executive who’d initially approved the “compliance model” got the call. The lead engineer stood in front of her Slack DM like a soldier awaiting orders.

“Wait,” she typed. “I thought this was handled.”

“Handled,” the engineer confirmed, sweating audibly through the keyboard.

“Then why am I getting a call from the Chief Compliance Officer?”

It turned out the failure had tripped a reporting trigger in the upstream data pipeline. The anomaly detection system—which no one realized was wired into the compliance reporting dashboard—had gone silent.

A compliance auditor had been monitoring the dashboard.

They saw a flatline where a graph was supposed to be.

“Is your anomaly detection down?” the email asked.  “No anomalies for 48 hours seems... unlikely.”

What engineering didn’t realize was that the dashboard wasn’t just a visualization—it was being used as a **compliance log record.** Under SOC-2 section 4.3.1(b), the organization was required to demonstrate “continuous monitoring and evidence of detection capability.”

The flatline wasn’t just “no anomalies.”

The flatline was being interpreted as **“no monitoring.”**

In other words:  \textbf{no anomalies → no alerts → no logs → no proof the system was functioning → failure of the control.}

The VP forwarded the email to the Chief Compliance Officer.

The Chief Compliance Officer forwarded it to Legal.

Legal forwarded it to Risk.

Risk forwarded it to the Board.

The Board requested a full compliance audit.

And somewhere, quietly, a Jira ticket labeled “low priority – intern project” was re-opened with a red flag icon.


\begin{quote}
In other words: a checked box had been unchecked by production reality.
\end{quote}

What started as a token model to placate an audit had become an existential compliance liability.

A third-party auditor was brought in.

\begin{itemize}

    \item They requested documentation.
    \item They requested training data.
    \item They requested logs of inference outputs.
    \item They requested a clear explanation of “feature2” versus “feature3.”
    \item They requested an explanation of the regression thresholds.
    \item They requested the model card.
    \item They requested proof the model even existed.

\end{itemize}

\textbf{There was none.}

The intern had built a monument to hubris.

And now it had become a monument to noncompliance.
    



\subsection{Act III: The Redemption (sort of...)}

    The audit had begun.
    
    Compliance demanded answers.
    
    Legal demanded timelines.
    
    Risk demanded mitigation.
    
    Executives demanded that someone—anyone—“own it.”

    \medskip

\begin{HistoricalSidebar}{The Legacy of SOX --- "Whatever You Do, Don't Tell Me"}

    When the Sarbanes-Oxley Act of 2002 (SOX) was passed in the wake of Enron and WorldCom, it didn’t just introduce stricter accounting rules—it rewired executive psychology.

    \medskip
    
    By making the \textbf{CEO and CFO personally liable} for knowingly certifying false financial statements (Sections 302 and 906), SOX created a powerful incentive:

    \medskip
    
    Not to know.

    \medskip
    
    Because knowledge wasn’t just power anymore.  
    \textbf{Knowledge was risk.}

    \medskip
    
    If an executive knew something was broken, misreported, or fraudulent—and signed the certification anyway—they weren’t just risking bad press.  
    They were risking prison.

    \medskip
    
    And so a new, quieter defense strategy took hold in boardrooms and corner offices across America:
    
    \begin{quote}
    “Whatever you do... don’t tell me.”
    \end{quote}
    
    Whistleblowers became legal liabilities.  
    Audit findings were radioactive.  
    The less an executive knew, the less they could be accused of concealing.

    \medskip
    
    Ironically, SOX’s attempt to increase accountability also fostered a culture of deliberate ignorance—where the highest-paid people in the room had a vested interest in staying out of the loop.
    
    \medskip
    
    \textit{Which explains something engineers have been complaining about for decades:} 
    When they say “leadership is clueless,” they’re absolutely right.

    \medskip
    
    What they don’t know is...  \textbf{it’s by design.}
    
\end{HistoricalSidebar}

    \medskip
    
    And so the team scrambled.
    
    They formed a ``model remediation task force''... which was really just the same engineers, in the same Slack channel, but with a new calendar invite.
    
    Every meeting began with the same three questions:
    
    \begin{enumerate}
        \item “What exactly broke?”
        \item “Why didn’t we know it would break?”
        \item “Who’s writing the slides for the board update?”
    \end{enumerate}
    
    At first, the intern was pulled into meetings as a formality.
    
    “Talk to the intern,” someone said in chat.
    
    “Ha, yeah — the intern’s the SME now,” someone else replied jokingly.

    \medskip

\begin{HistoricalSidebar}{“Subject Matter Expert”—A Title Bestowed, Not Earned}

In theory, a \textbf{Subject Matter Expert (SME)} is someone with deep, specialized knowledge in a particular domain.  

\medskip

In practice, an SME is whoever’s name gets typed into the chat when a question goes unanswered.

\medskip

The term emerged alongside the rise of formalized knowledge management in the late 20th century, as corporations sought to map expertise onto org charts. But somewhere along the way, the process inverted:  

\medskip

Expertise wasn’t something you proved; It was something you were \textit{labeled}.

\begin{quote}
“The intern? Yeah, they’re the SME now.”
\end{quote}

And so were born titles like \textit{Cloud Strategist}, \textit{Cloud-Native Strategist}, \textit{Hybrid-Cloud Engineer}, \textit{Multi-Cloud Engineer}, \textit{Innovation Visionary}, and \textit{AI Transformation Lead}: roles defined less by measurable skill, and more by the need to write something on a conference badge.

\medskip

In corporate lore, being an SME isn’t about credentials or mastery.  It’s about being the last person standing when the spotlight turns.  

\begin{quote}
Congratulations. You’re the expert now.
\end{quote}

\end{HistoricalSidebar}

    \medskip
    
    But then it stuck.
    
    And once it stuck, it metastasized.
    
    He wasn’t just an intern anymore.
    
    He was the “subject matter expert.”
    
    \textit{Officially.}
    
    The title was written into the remediation report.  
    The title was printed on the org chart.  
    The title was spoken aloud at the compliance review.
    
    And once compliance had a name --- his name --- they didn’t care who the actual lead engineer was.
    
    Every question flowed to him.
    
    Every email CC’d him.
    
    Every Slack mention @’ed him.
    
    When legal asked “Can you confirm the regression threshold was compliant with SOC-2 subsection 4.3.1(b)?”  

    His response was “...What’s SOC-2 subsection 4.3.1(b)?”
    
    When SecOps asked “Did we validate drift against production telemetry?”  
    
    His response was “...What’s telemetry?”
    
    When Risk Management asked “Do you have an updated data lineage diagram?”  

    His response was “...I ...I have a CSV?”
    
    At first, he answered from memory.
    
    Then he realized memory wasn’t enough.
    
    Compliance wasn’t going away.
    
    \textbf{They wanted real answers.}
    
    So he had to find them.
    
    He spent the next three weeks talking to every team that might be upstream.
    
    Analytics.  
    Observability.  
    Site Reliability.  
    Data Engineering.  
    An ancient team known only as “Telemetry Ops,” reachable only via an unmaintained group email.
    
    He read dashboards no one had logged into for years.  
    He traced cronjobs that hadn’t been touched since the last reorg.  
    He discovered the telemetry logs weren’t coming from a single system, but from six different services stitched together with sidecar scripts and duct tape.
    
    And little by little, he learned.
    
    Not because he wanted to.
    
    Not because it was his job.
    
    But because once compliance wrote his name down,  
    he had no choice.
    
    The intern wasn’t the scapegoat anymore.
    
    He was the face.
    
    He wasn’t just “responsible.”
    
    He was accountable.
    
    And by the time the audit concluded, the outcome was inevitable: the company hired him.
    
    \begin{quote}
    They didn’t hire him because he was brilliant.
    
    They hired him because—  
    by accident, by inertia, by the quiet violence of institutional entropy—  
    he had become the last living person who understood how the telemetry fed into the compliance pipeline.
    \end{quote}
    
    The model wasn’t just a model anymore.
    
    It had been referenced in quarterly filings.  
    It had been cited in internal audit trails.  
    It had been exported, as documentation, to an official government oversight body.
    
    And because no one else dared touch it—  
    because no one else knew how the system fit together—  
    the intern wasn’t just an engineer.
    
    He was infrastructure.
    
    And so he stayed.
    
    A little older.
    
    A little quieter.
    
    A little more haunted.
    
    He learned to wear headphones in meetings.  
    He learned to keep his calendar blocked with “important meetings.”  
    He learned never to sit near HR at the all-hands.
    
    The box was checked.
    
    The dashboard was green.
    
    But somewhere, deep in the telemetry logs,  
    another flag—  
    a new flag—  
    was quietly being written.
    
    No one knew what it meant.
    
    No one wanted to ask.
    
    And only the SME knew how to fix it.

    




    \subsection{A Case Study in Systemantics: Law 2 and Law 4 in Action}

    The story of the intern wasn’t just a tale of bad luck or oversight. It was a textbook demonstration of two key principles from John Gall’s \textit{Systemantics}:
    
    \begin{enumerate}
        \item \textbf{Law 2: “A complex system that works is invariably found to have evolved from a simple system that worked.”}
        \item \textbf{Law 4: “The real-world functioning of a system is never what its designer intended.”}
    \end{enumerate}
    
    The compliance model was never designed to be a compliance system.  
    It was a checkbox.  
    A stopgap.  
    A throwaway script meant to satisfy an auditor’s brief glance.
    
    But even a throwaway script is still a system.  
    And like all systems, it evolved.
    
    \medskip
    
    The moment it was deployed into production, the model became embedded inside operational workflows.  
    Its outputs fed dashboards.  
    Those dashboards became compliance evidence.  
    Those compliance dashboards became quarterly filings.
    
    \begin{quote}
    What began as a simple “yes, we have anomaly detection” turned into a fragile dependency no one could replace without rebuilding the whole thing.
    \end{quote}
    
    The system didn’t evolve by design.  
    It evolved by inertia.
    
    Each patch, each manual upload, each undocumented workaround added complexity without adding robustness.  
    Every layer buried the system deeper inside institutional processes.
    
    And so, by the time the audit arrived, it wasn’t a script anymore.  
    It was infrastructure.  
    It was compliance.  
    It was \textbf{the system}.
    
    \medskip
    
    \textbf{And that’s where Law 4 reared its head:}  
    The real-world functioning of the system had nothing to do with what its creators intended.
    
    No one intended for an intern’s hastily written script to become mission-critical.  
    No one intended for telemetry logs of unknown origin to feed compliance dashboards.  
    No one intended for SOC-2 controls to hinge on a CSV exported by hand every month.
    
    \begin{quote}
    But the system didn’t care what they intended.
    It only cared what they implemented.
    \end{quote}
    
    \medskip
    
    \textbf{The root failure wasn’t just technical—it was structural.}
    
    The team had centralized the entire compliance pipeline in a single monolith, operated by a single engineer, without modularity or separation of concerns.
    
    Had they used a more modular architecture—like an Airflow DAG or another orchestrated workflow—they could have decoupled the stages:
    
    \begin{itemize}
        \item Data ingestion
        \item Data preprocessing
        \item Model training
        \item Model validation
        \item Model deployment
        \item Compliance reporting
    \end{itemize}
    
    Each stage could have been owned by a different team, reviewed by domain experts, version-controlled independently, and validated against changing requirements.
    
    Instead, every function was locked inside a single script, a single person, a single point of failure.
    
    \medskip
    
    \textbf{And so, when it failed—it failed all at once.}
    
    The system wasn’t modular.  
    It wasn’t transparent.  
    It wasn’t adaptable.
    
    It was brittle.  
    And when brittle systems break, they don’t degrade gracefully.  
    They shatter.
    
    \medskip
    
    \begin{quote}
    The real tragedy?  
    They didn’t inherit a complex system.
    They built one—from a simple one—by accident.
    \end{quote}
    
    And because they never re-architected it, they guaranteed that complexity would ossify rather than evolve.
    
    A lesson written not just in Systemantics, but now, in their own compliance audit.
    


    \subsection{Aviation Knows: Why DO-178C Could Have Saved Them}

    While the compliance catastrophe at the company unfolded as a textbook failure of software system design, it’s worth noting that entire industries have already solved this category of problem — because they had no choice.
    
    Enter \textbf{DO-178C}, the international standard governing airborne software systems.
    
    \medskip
    
    DO-178C doesn’t just demand that you have software that “works.”  
    It demands that you can \textbf{prove}, through rigorous, auditable processes, that each piece of software:
    
    \begin{itemize}
        \item Has traceable requirements.
        \item Has documented design.
        \item Has verified and validated implementation.
        \item Has test cases tied explicitly to requirements.
        \item Passes those test cases under controlled, repeatable conditions.
    \end{itemize}
    
    This is not about vague best practices or aspirational engineering values.  
    This is about survival.
    
    \medskip
    
    In aviation, the failure of a single undocumented script doesn’t just cause a Slack firestorm — it can kill people.  
    That’s why DO-178C emphasizes:
    
    \begin{itemize}
        \item \textbf{Module-level testing}: Each software unit (module) must be verified independently, with clear interfaces and defined functionality.
        \item \textbf{Requirements-based coverage}: You don’t just write tests to increase code coverage — you write them to satisfy explicitly documented requirements.
        \item \textbf{Robust traceability}: Every line of code is tied to a requirement, and every requirement is tied to a verification activity.
        \item \textbf{Change impact analysis}: If the data format changes, or a field disappears, or a flag mutates, the system forces you to analyze how that change propagates through every module and interface.
    \end{itemize}
    
    \medskip
    
    \begin{quote}
    \textbf{In other words: you can’t YOLO an intern script into production — because there’s no such thing as “just a temporary solution” in safety-critical software.}
    \end{quote}
    
    \medskip
    
    Had the company applied even a fraction of DO-178C’s rigor — even if not formally required by their domain — they would have caught:
    
    \begin{itemize}
        \item The absence of modular boundaries.
        \item The lack of interface definitions.
        \item The missing test cases around log format changes.
        \item The undocumented assumptions about feature flags.
        \item The total absence of traceability between inputs, models, outputs, and compliance claims.
    \end{itemize}
    
    \medskip
    
    \textbf{Most crucially:} module-level tests would have allowed them to isolate failures.  
    Instead of the entire system collapsing when a log format shifted, a properly tested preprocessing module would have raised an early, localized failure — which could have been corrected without dragging the entire compliance apparatus down with it.
    
    \medskip
    
    \begin{quote}
    \textit{A complex system is not dangerous because it is big.
    It is dangerous because its parts are invisible, untested, and intertwined.}
    \end{quote}
    
    DO-178C offers a blueprint for breaking that cycle.  
    Not because it guarantees perfection — but because it enforces separation of concerns, documented interfaces, and module-level accountability.
    
    \medskip
    
    In short:  
    \textbf{It’s a roadmap for turning legacy spaghetti into an engineered system.}
    
    And even outside aviation, companies with critical dependencies would do well to study its lessons.
    












