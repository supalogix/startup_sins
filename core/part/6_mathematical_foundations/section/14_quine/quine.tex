






\section{Control as Inference: The Variational View of Optimal Action (1992--2003)}

\subsection{From Foundations to Functions: Living with Uncertainty}

If Gödel cracked the foundation of formal systems, then the decades that followed were a slow reckoning with what that meant. Certainty—the bedrock ambition of Hilbert, the Vienna Circle, and early 20th-century science—was gone. In its place rose a new intellectual attitude: not one of defeat, but of adaptation. If truth could no longer be captured in a closed system, perhaps it could still be approached through approximation. If we couldn’t guarantee completeness, we could optimize for coherence, for consistency, for usefulness.

The shift wasn’t just philosophical. It was computational.

Rather than asking what is ultimately provable, scientists and engineers began asking what is \textbf{computationally feasible}. What can we model? What can we predict? What can we control? Probability theory, once treated as the ugly duckling of epistemology, became the new language of uncertainty. And inference—no longer a mechanical derivation from axioms—became an act of estimation under constraints.

Enter the variational view.

Where Gödel exposed the limits of formal systems, variational inference embraced those limits. It replaced universal proof with \emph{best-fit approximation}, logical certainty with \emph{probabilistic compression}, and deductive closure with \emph{evidence maximization}. In this world, truth is no longer binary. It’s a distribution. A ranking of plausibilities. A spectrum of beliefs conditioned on data, structure, and prior knowledge.

This isn’t a retreat from rigor—it’s a reframing of it. Optimization becomes the new logic. Compression becomes the new proof. And inference becomes an adaptive, dynamic act: not of asserting what must be true, but of \textit{inferring what is most probable, given what we know}.

\noindent
With this turn, the story pivots—from broken certainty to functional reasoning. From foundational cracks to inferential bridges. From Gödel... to MacKay.

\begin{tcolorbox}[colback=gray!5!white, colframe=black!75!white, title={Historical Sidebar: W.V.O. Quine and the Death of the Analytic-Synthetic Divide}]

    \textbf{Willard Van Orman Quine} (1908–2000) was one of the most influential American philosophers of the 20th century—especially in logic, epistemology, and the philosophy of science.

    \medskip
    
    Early in his career, Quine was deeply influenced by the \textbf{Vienna Circle}, the group of logical positivists who sought to reduce all meaningful statements to either logical tautologies or empirically verifiable observations. He studied with \textbf{Rudolf Carnap} in Prague and was sympathetic to their vision: a purified language of science, grounded in logic and empiricism.

    \medskip
    
    But that early allegiance didn’t last.
    
    \medskip
    
    \textbf{Gödel’s incompleteness theorems}, which showed that no consistent formal system could capture all truths of arithmetic, rattled the foundations of that logical dream. While the Vienna Circle largely tried to sidestep Gödel’s results as technical nuisances, Quine took them seriously. He saw them as symptoms of a deeper problem: the idea that logic and meaning could be cleanly separated from empirical content.

    \medskip
    
    In his landmark 1951 essay, \textit{“Two Dogmas of Empiricism,”} Quine dismantled one of the core tenets of logical positivism: the distinction between \textbf{analytic truths} (true by virtue of meaning) and \textbf{synthetic truths} (true by virtue of experience). He argued that:

    \medskip
    
    
    \begin{itemize}
        \item There is no clear boundary between logical truths and empirical claims,
        \item Our beliefs form a holistic web, tested as a system rather than individually,
        \item Even mathematics and logic are, in principle, revisable in light of experience.
    \end{itemize}

    \medskip
    
    \textbf{This was a philosophical earthquake.}

    \medskip
    
    By rejecting the analytic-synthetic distinction, Quine helped bury the Vienna Circle's program. In its place, he proposed a more pragmatic, naturalistic epistemology: science doesn’t rest on indubitable foundations—it evolves as a network of beliefs, subject to revision and optimization.
    
    \medskip
    
    \textbf{Gödel cracked formalism. Quine dismantled its philosophical scaffolding.}
    
\end{tcolorbox}



\subsection{Maximum Evidence and the Birth of Variational Inference}

The modern view of control as inference owes much to a deeper revolution in probabilistic modeling: the development of \textbf{variational inference}, and in particular, its cornerstone—the \emph{variational principle of maximum evidence}. This idea reframes inference not as exact computation over intractable posteriors, but as optimization: choose the best approximation from a restricted family by maximizing the model's ability to explain observed data.

At the heart of this shift was \textbf{David MacKay}, whose influential work in the 1990s and early 2000s linked Bayesian reasoning, statistical physics, and information theory into a unified computational principle. In his landmark book, \emph{Information Theory, Inference, and Learning Algorithms}, MacKay emphasized that learning should not aim to maximize likelihood directly, nor to minimize loss in isolation, but rather to \emph{maximize the evidence}—the marginal likelihood of the data under the model.

This approach, also known as \emph{Type-II Maximum Likelihood}, selects models not just for fit, but for parsimony: simpler models are favored unless the data strongly suggest otherwise. The resulting optimization yields the \textbf{Evidence Lower Bound (ELBO)}—a tractable proxy for the log-evidence—whose maximization leads to both an approximate posterior and a data-driven measure of model quality.

In this framework, inference becomes an act of compression: how efficiently can we represent the data in terms of latent structure? This same logic, when applied to control, flips the perspective: how efficiently can we compress future trajectories into desirable outcomes, given uncertainty? MacKay’s principle, originally designed for probabilistic coding and neural networks, now guides a growing body of work that treats \emph{action selection as probabilistic inference}, grounded in the same variational machinery.

The next section explores how this principle—born in Bayesian inference—was adapted by control theorists to infer not just what is true, but what should be done.


\subsection{From Trajectories to Beliefs: A Probabilistic Reframing}

In classical control theory, the central question is: \emph{what should I do to minimize a cost?}  
In Bayesian inference, the central question is: \emph{what should I believe given what I’ve observed?}  

At first glance, these seem like entirely different problems—one is about acting, the other is about knowing. But in the late 20th century, researchers began to realize something remarkable: these problems are not opposites. They are duals.

The turning point came when control theorists and probabilists began using the same tools—specifically, the calculus of variations, entropy maximization, and measure theory. This convergence gave rise to a new paradigm: \textbf{control as inference}.

In this view, the task of choosing optimal actions is reframed as an inference problem. Instead of asking, “What action minimizes my cost?”, we ask, “What trajectory of states and actions is most probable, under a distribution that favors both feasibility and low cost?”

\subsection{The Variational Principle of Maximum Evidence}

This approach was formalized through a method known as \textbf{variational inference}—a technique that emerged in the 1980s and 1990s in the context of probabilistic graphical models, and was refined into a general computational framework by researchers like Geoffrey Hinton, Michael Jordan, and later Kevin Murphy and Sergey Levine in control settings.

At its core, variational inference approximates an intractable posterior distribution \( p(z|x) \) by a simpler distribution \( q(z) \). The goal is to minimize the Kullback–Leibler divergence between them:
\[
\text{KL}(q(z) \,||\, p(z|x)) = \int q(z) \log \frac{q(z)}{p(z|x)} \, dz
\]

Since \( p(z|x) \) is often unknown or hard to compute directly, variational methods reframe the problem by maximizing a lower bound on the \textbf{log evidence}, also called the \emph{Evidence Lower BOund (ELBO)}:
\[
\log p(x) \geq \mathbb{E}_{q(z)}[\log p(x|z)] - \text{KL}(q(z) \,||\, p(z))
\]

This is where control theory reenters the story.

Suppose we want to find an optimal control trajectory \( u(t) \) that steers a system toward some outcome while incurring low cost. In the inference-as-control view, we:

\begin{itemize}
  \item Define a probability distribution over trajectories, where low-cost trajectories are more probable.
  \item Infer the most likely trajectory using variational inference.
\end{itemize}

Now the “best” control sequence becomes the \textbf{MAP estimate} (maximum a posteriori) of a probability distribution over trajectories. Optimization becomes inference.

\subsection{From Cost Functionals to Probabilistic Action}

To make this work, we reinterpret cost as \emph{log-probability}. Given a path \( \tau = (x_{0:T}, u_{0:T}) \), we define a “soft” cost objective as a probability:
\[
p(\tau) \propto \exp\left( - J[\tau] \right)
\]
where \( J[\tau] \) is the classical cost functional.

This Boltzmann-like distribution says: paths with lower cost are exponentially more likely. It’s the same mathematical form as in statistical mechanics and maximum entropy models.

This lets us frame control as inference over trajectories:
\[
q^*(\tau) = \arg\min_q \text{KL}(q(\tau) \,||\, p(\tau))
\]
—an instance of variational inference.

\subsection{Learning to Act by Learning to Infer}

This reframing has profound consequences. It means that control systems can learn not by optimizing hard-to-solve equations directly, but by simulating posterior distributions and sampling optimal behaviors.

In this paradigm:
\begin{itemize}
  \item The cost function becomes a likelihood term.
  \item The dynamics become a prior.
  \item The policy becomes an inference model.
\end{itemize}

And crucially, the solution is no longer deterministic. Just like Bayesian inference doesn’t yield a single true belief but a distribution of possibilities, optimal control now yields a \emph{distribution over actions}. Uncertainty is no longer a nuisance—it is built into the logic of decision.

\subsection{Entropy, Information, and the Geometry of Control}

The geometry of variational control is governed by the same principles we’ve seen before:

\begin{itemize}
  \item The best action is the one that reduces future uncertainty.
  \item The best belief is the one that explains observed outcomes with minimal bias.
  \item The best trajectory is the one that balances likelihood and cost.
\end{itemize}

In this view, the entropy of a trajectory distribution becomes a measure of flexibility; the KL divergence becomes a penalty for deviating from prior expectations; and the variational principle becomes the bridge between knowledge and action.

\begin{quote}
\textit{To act optimally is to infer structure. To infer structure is to act optimally.}
\end{quote}

From Pontryagin’s Hamiltonian to Shannon’s entropy, from Boltzmann’s weights to Bayesian belief—variational inference is the unifying thread. It doesn’t just approximate inference. It reinterprets control itself as the art of informed uncertainty.


\subsection{When KL Divergence Misbehaves: The Role of Absolute Continuity}

In variational inference, the optimization objective typically involves minimizing the Kullback–Leibler (KL) divergence between a variational distribution \( q(z) \) and the true posterior \( p(z \mid x) \):

\[
\mathrm{KL}(q(z) \,\|\, p(z \mid x)) = \int q(z) \log \frac{q(z)}{p(z \mid x)} \, dz
\]

This divergence plays a central role in deriving the Evidence Lower Bound (ELBO):

\[
\log p(x) = \mathbb{E}_{q(z)}[\log p(x, z) - \log q(z)] + \mathrm{KL}(q(z) \,\|\, p(z \mid x))
\]

Minimizing the KL divergence corresponds to maximizing the ELBO. However, for the KL divergence to be well-defined and finite, it requires a crucial measure-theoretic condition:

\subsubsection{Absolute Continuity is Required}

KL divergence is only defined if the variational distribution \( q \) is \textbf{absolutely continuous} with respect to the posterior \( p \). This means:

\[
q \ll p \quad \text{(i.e., if } p(A) = 0 \text{ then } q(A) = 0 \text{ for all measurable sets } A)
\]

If this condition is violated:
\begin{itemize}
    \item The ratio \( \frac{q(z)}{p(z)} \) becomes undefined on some sets.
    \item The KL divergence becomes infinite or entirely undefined.
\end{itemize}

\subsubsection{KL Can Still Be Infinite}

Even when absolute continuity holds, the KL divergence may still diverge:

\[
\mathrm{KL}(q \,\|\, p) = \int q(z) \log \frac{q(z)}{p(z)} \, dz
\]

If \( p(z) \) is very small (approaching zero) where \( q(z) \) is non-negligible, the log-term can go to \( +\infty \), causing the integral to diverge.

\subsubsection{Consequences for Variational Inference}

\begin{itemize}
    \item Choosing variational distributions with compact support while the true posterior has heavy tails can cause KL to be undefined or infinite.
    \item In VAEs and amortized inference, Gaussian variational distributions may not overlap well with multimodal or heavy-tailed posteriors.
    \item Even when KL is technically finite, poor support overlap can lead to high variance gradients and unstable training.
\end{itemize}

\subsubsection{Theoretical Implication: Bayes' Rule as Radon–Nikodym Derivative}

KL divergence implicitly relies on Bayes' rule, which in measure-theoretic terms is:

\[
\frac{dP(\theta \mid x)}{dP(\theta)} = \frac{p(x \mid \theta)}{p(x)}
\]

This derivative only exists if \( P(\theta \mid x) \ll P(\theta) \), i.e., the posterior is absolutely continuous with respect to the prior. The same logic applies in the reverse for variational inference: KL divergence assumes absolute continuity of \( q \) with respect to \( p \).

\subsubsection{Summary Table}

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Condition} & \textbf{Outcome} \\
\hline
\( q(z) \ll p(z \mid x) \) & KL well-defined \\
\( q(z) \not\ll p(z \mid x) \) & KL undefined or infinite \\
\( p(z \mid x) = 0 \) where \( q(z) > 0 \) & Division by zero in KL \\
Variational family too narrow & KL misrepresents true divergence \\
Heavy-tailed or multimodal true posterior & KL mismatch and underfitting risk \\
\hline
\end{tabular}
\end{center}

\subsubsection{Workarounds and Fixes}

\begin{itemize}
    \item Use alternative divergences: \(\alpha\)-divergences, Rényi divergence, or reverse KL.
    \item Use \textbf{implicit variational inference} where the density of \( q \) is not needed.
    \item Employ \textbf{normalizing flows} to enrich the variational family and improve support matching.
\end{itemize}



\subsection{Measure-Theoretic Foundations of Variational Information}

Variational information methods, such as those used in the Information Bottleneck, VAEs, and mutual information estimators, rely heavily on KL divergence to approximate or bound mutual information. These methods inherit all of the measure-theoretic assumptions that KL divergence requires to be valid and finite.

\subsubsection{Mutual Information and Variational Bounds}

Mutual information between random variables \( X \) and \( Z \) is defined as:

\[
I(X; Z) = \mathrm{KL}(p(x, z) \,\|\, p(x)p(z)) = \mathbb{E}_{p(x)} \left[ \mathrm{KL}(p(z \mid x) \,\|\, p(z)) \right]
\]

Since this quantity is often intractable, variational approximations are introduced:

\[
I(X; Z) \leq \mathbb{E}_{p(x)} \left[ \mathrm{KL}(q(z \mid x) \,\|\, q(z)) \right]
\]

This variational upper bound is the basis of many information-theoretic objectives in machine learning, such as:

\begin{itemize}
    \item The \textbf{Information Bottleneck} principle:
    \[
    \min I(X; Z) - \beta I(Z; Y)
    \]
    \item The \textbf{ELBO} in VAEs:
    \[
    \mathrm{ELBO} = \mathbb{E}_{q(z \mid x)}[\log p(x \mid z)] - \mathrm{KL}(q(z \mid x) \,\|\, p(z))
    \]
\end{itemize}

\subsubsection{The Role of Absolute Continuity}

To compute the KL divergence \( \mathrm{KL}(q(z \mid x) \,\|\, q(z)) \), we require that:

\[
q(z \mid x) \ll q(z)
\]

This ensures that the density ratio \( \frac{q(z \mid x)}{q(z)} \) is well-defined almost everywhere. If this condition fails:

\begin{itemize}
    \item The KL divergence becomes infinite or undefined.
    \item The variational upper bound on mutual information is invalid.
    \item Optimization using the ELBO or information bottleneck may fail or become unstable.
\end{itemize}

\subsubsection{Pathologies in Variational Approximations}

\begin{itemize}
    \item If \( q(z \mid x) \) places mass where \( q(z) = 0 \), KL is undefined.
    \item If \( q(z) \) is too broad or too narrow, KL may be misleading or divergent.
    \item In VAEs, poor support overlap can cause \textbf{posterior collapse}, where \( I(X; Z) \to 0 \).
    \item In MINE (Mutual Information Neural Estimation), the lower bound fails if \( p(x, z) \not\ll p(x)p(z) \).
\end{itemize}

\subsubsection{Improper Priors and Marginals}

Many models assume priors like \( p(z) \propto 1 \), which are improper (not normalizable). In such cases:

\begin{itemize}
    \item \( \mathrm{KL}(q(z \mid x) \,\|\, p(z)) \) may be undefined.
    \item The mutual information becomes non-computable.
    \item The variational objective loses grounding in probability theory.
\end{itemize}

\subsubsection{Summary Table}

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Issue} & \textbf{Consequence} \\
\hline
\( q(z \mid x) \not\ll q(z) \) & KL undefined or infinite \\
Support mismatch & Information estimates fail \\
Improper prior \( p(z) \) & KL becomes non-integrable \\
Posterior collapse & \( I(X; Z) \to 0 \), no useful representation \\
Variational family too narrow & Underestimates true mutual information \\
\hline
\end{tabular}
\end{center}

\subsubsection{Practical Implications and Fixes}

\begin{itemize}
    \item Use \textbf{alternative divergences}, e.g., Rényi or reverse KL.
    \item Employ \textbf{normalizing flows} to ensure better support overlap.
    \item Use \textbf{implicit variational inference} methods that don’t require evaluating densities.
    \item Carefully construct variational families to match the support and shape of true posteriors.
\end{itemize}

\subsubsection{Foundational View}

Ultimately, all these issues stem from the measure-theoretic core of information theory. Variational methods quantify information by comparing distributions—and if those distributions aren’t properly related through absolute continuity, the very notion of "information" becomes meaningless in practice.

\begin{quote}
To measure information, the approximating and true distributions must live in the same mathematical universe.
\end{quote}


