\section{Amari: Geometry Learns to Think}

\subsection{Shun-ichi Amari: Curvature in the Space of Belief}

If Jeffreys gave us belief as integration, and Jordan gave us belief as optimization, then Shun-ichi Amari gave us belief as \emph{geometry}.

Where Michael Jordan treated KL divergence as a functional to minimize, Amari treated it as a kind of \emph{distance}—but not in the usual Euclidean sense. Instead, he showed that KL divergence defines a local curvature on the space of probability distributions, encoded by the Fisher Information metric. In this geometric view, learning becomes motion: belief updates trace geodesics across a curved manifold of possible models.

This is the heart of \textbf{information geometry}—a field Amari pioneered—where statistical inference is governed by differential geometry. Gradients become \emph{natural gradients}, adjusted for the curvature of the belief space, allowing more efficient and principled learning updates.

\[
I(\theta) = \int f(x \mid \theta) \left( \frac{\partial \log f(x \mid \theta)}{\partial \theta} \right)^2 dx
\]

This same Fisher Information that Jeffreys used to define his prior, and that Jordan used to characterize variational families, becomes in Amari’s hands a full-blown \textbf{Riemannian metric}. Geometry and learning are no longer separate. They are one and the same.


\begin{tcolorbox}[colback=yellow!5!white, colframe=yellow!50!black, title=Ernst Mach and the Sensory Roots of Inference]
    Before Bayesian inference found its natural gradient, it found its natural habitat—in the physiology of perception.
    
    \textbf{Ernst Mach}, a 19th-century physicist-philosopher, argued that the laws of physics (and thought itself) arise not from some external metaphysics, but from the way we experience the world through sensation. He believed cognition was fundamentally shaped by regularities in sensory input, not by abstract reason.
    
    This idea—that inference is grounded in perception—foreshadowed modern models of learning as \emph{adaptive geometry}. Mach’s emphasis on sensory experience as the bedrock of knowledge resonates deeply with \textbf{Shun-ichi Amari’s} later work in information geometry, where learning systems adapt not just to data, but to the curvature of the belief space they inhabit.
    
    In a sense, Mach replaced Newton’s laws with perceptual laws.  
    Amari did the same—replacing logic with landscape.
\end{tcolorbox}


\subsection{From Mutual Dependence to Expected Surprise}

Originally defined as a double sum, mutual information too became an expectation over divergence:

\[
I(X; Y) = \int p(y) \left[ \int p(x \mid y) \log \frac{p(x \mid y)}{p(x)} \, dx \right] dy
\]

This nested integral has no closed form in most real-world applications. Computing it is hard—but its interpretation is beautiful: it is the average surprise you’d feel about \( X \) after learning \( Y \).

Mutual information is what powers modern learning to ask not “what’s true?” but “what’s informative?”

\subsection{From Precision to Principle: Fisher Information Reused}

Originally introduced by Ronald Fisher as a bound on estimator variance, Fisher Information quantified how sensitive a likelihood function was to changes in its parameters. It told you how much the data “sharpens” your estimate—how much precision you gain from observation. In this form, it entered Jeffreys’ thinking as a way to define a prior that was invariant under reparametrization:

\[
\pi(\theta) \propto \sqrt{I(\theta)} \quad \text{with} \quad I(\theta) = \int f(x \mid \theta) \left( \frac{\partial \log f(x \mid \theta)}{\partial \theta} \right)^2 dx
\]

But Shun-ichi Amari saw something deeper in this formula.

To Amari, the Fisher Information wasn’t just a measure of precision—it was the local curvature of belief space. It defined a Riemannian metric on the manifold of probability distributions, turning statistical inference into a problem of geometry. This reinterpretation gave birth to \textbf{information geometry}.

In this framework, nearby distributions are “close” if their KL divergence is small. And just as Euclidean geometry measures distance with a dot product, information geometry measures divergence with Fisher Information:

\[
D_{\mathrm{KL}}(p(\theta + d\theta) \parallel p(\theta)) \approx \frac{1}{2} d\theta^T I(\theta) d\theta
\]

This second-order approximation shows that Fisher Information plays the same role as a metric tensor—it defines the inner product on the tangent space of probability distributions.

Inference, in Amari’s hands, becomes movement through curved space. Ordinary gradients are replaced with \textbf{natural gradients} that follow the geodesics of this space, adjusting not just for slope but for shape. Learning becomes a kind of navigation—one that honors the geometry of uncertainty.

What began as a technical tool for bounding error becomes, in this light, a unifying principle:  
Fisher Information doesn’t just measure precision—it defines the structure of belief itself.

\begin{tcolorbox}[colback=purple!5!white, colframe=purple!60!black, title=Penrose and the Shape of Thought]

    \textbf{Roger Penrose}, renowned for his work in general relativity and the geometry of spacetime, believed that geometry wasn't just a tool for physics—it was a window into the deep structure of reality. From black holes to twistor theory, Penrose treated curvature not as an artifact, but as a first principle.
    
    In this light, information geometry reads like a distant cousin of Penrose’s vision. Where he described gravity as curvature in the fabric of spacetime, information geometry describes learning as curvature in the fabric of belief. The \textbf{Fisher Information Metric} plays the same role that the Einstein tensor does: it tells you how structure bends in response to uncertainty or signal.
    
    Yet Penrose went further. In \textit{The Emperor’s New Mind}, he questioned whether algorithmic systems—even those grounded in elegant geometry—could fully account for consciousness. He suspected something deeper: a non-computable foundation for awareness, perhaps encoded in quantum geometry itself.
    
    So while \textbf{Amari’s manifolds} guide machines to learn, and \textbf{Penrose’s spacetime} explains how matter moves, both suggest a profound truth:
    
    \begin{quote}
    \emph{To understand structure is to understand flow. To understand flow is to understand mind.}
    \end{quote}
    
    And maybe—just maybe—geometry is the ghost in the machine.
\end{tcolorbox}


\subsection{From Spacetime to Beliefspace: The Riemann–Amari Parallel}

When Riemann introduced his metric tensor to generalize geometry beyond Euclid, he didn’t just bend space—he changed how we think about structure itself. No longer was distance fixed or flat. It became a local phenomenon, encoded by curvature. In physics, this laid the groundwork for general relativity, where the geometry of spacetime is shaped by mass and energy.

Shun-ichi Amari did something uncannily similar—but with probability instead of position.

In his formulation, the space of all possible probability distributions becomes a \emph{manifold}, and the Fisher Information defines its local curvature. Where Einstein used the Riemann tensor to describe how spacetime bends in the presence of mass, Amari used the Fisher metric to describe how \emph{inference bends} in the presence of information.

\begin{center}
\textit{A bit like Riemann’s move with metrics, but with entropy instead of curvature.}
\end{center}

This isn’t just analogy—it’s mathematics. The second-order approximation of KL divergence mirrors the structure of a Riemannian metric:

\[
D_{\mathrm{KL}}(p(\theta + d\theta) \parallel p(\theta)) \approx \frac{1}{2} d\theta^T I(\theta) d\theta
\]

In this equation, \( I(\theta) \) isn’t just a matrix—it’s a lens. It shapes the space of beliefs, telling us how to measure distance between models not by coordinate difference, but by informational distinguishability.

So just as general relativity revealed that gravity is geometry, Amari’s framework suggests that learning is geometry too.

\begin{quote}
To optimize is to move.  
To learn is to move wisely.  
And to move wisely, you must know the shape of the space you're in.
\end{quote}

In that sense, Amari didn’t just teach machines to learn: he taught learning how to see.
