\subsection{Entropy, Measure, and the Shadow of Uncertainty}

The power of Pontryagin’s Maximum Principle lies not only in its ability to optimize motion, but in its philosophical contrast to another great pillar of 20th-century mathematics: entropy. Where Kolmogorov and Shannon built a framework around uncertainty—modeling systems through distributions, randomness, and inference—Pontryagin constructed a geometry of control, where systems are navigated with intent, not passively observed but actively steered.

At first glance, these two worlds seem far apart: one stochastic, the other deterministic; one probabilistic, the other geometric. But both rely on a shared formalism—**the integral**—as a way to encode global structure across infinite possibilities:
\[
\text{Entropy:}\quad H(X) = - \int p(x) \log p(x) \, dx
\qquad \text{vs.} \qquad
\text{Cost:}\quad J = \int L(x(t), u(t)) \, dt
\]

In information theory, the integral measures expected surprise—the degree to which an outcome defies expectation. In control theory, it measures accumulated cost—the penalty for deviation from an optimal path. Both reflect a universe constrained by optimization, but they imply different epistemologies: one in which we infer the world by maximizing uncertainty (entropy), the other in which we act upon it by minimizing loss (cost).

This contrast echoes a deeper intellectual tension. Kolmogorov’s mathematical universe was probabilistic and abstract, a world in which one infers structure from noisy observations. His entropy formalism was agnostic, almost epistemologically humble: it did not presume to control the system, only to characterize what could be known. Pontryagin, by contrast, offered a world of levers and feedback—where costates, not probabilities, governed behavior. His equations presumed agency, purpose, design.

In this light, Pontryagin’s costate vector \( p(t) \) can be seen as the deterministic analogue of Bayesian sensitivity. It propagates backward in time, measuring how future objectives constrain present motion—just as a posterior in Bayesian inference reshapes our beliefs in light of evidence. The adjoint equation:
\[
\dot{p}(t) = -\frac{\partial H}{\partial x}
\]
is a kind of causal retrograde: it tells us how today's state must be adjusted to preserve tomorrow's optimality.

Both frameworks encode feedback. But where Kolmogorov’s feedback loop updates belief, Pontryagin’s adjusts behavior. One models uncertainty; the other minimizes it. And in the Soviet context, this distinction mattered. Kolmogorov’s probabilistic frameworks, with their open-ended abstraction, sat uneasily within a political system that demanded certainty, planning, and output. Pontryagin’s deterministic control, with its actionable gradients and directed goals, was a mathematics more compatible with the state.

Thus, beneath the symbols and integrals lies a historical and philosophical divergence: two visions of what mathematics is for—whether to describe the world as it is, or to shape it as it ought to be.