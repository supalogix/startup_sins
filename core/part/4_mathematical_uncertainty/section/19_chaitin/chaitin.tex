\section{When Logic Breaks: Gödel, Chaitin, and the Limits of Formal Reasoning}

Concentration inequalities give us a kind of practical certainty. They let us bound fluctuations, estimate deviations, and place bets on the likely behavior of random systems—even when full knowledge is out of reach. But what happens when the uncertainty lies not in the data, but in the structure of logic itself?

This question leads us away from probability and into the heart of mathematical foundations.

In 1931, \textbf{Kurt Gödel} shook the world of mathematics with his \emph{incompleteness theorems}. His result was simple and devastating: in any sufficiently powerful formal system, there exist true mathematical statements that can never be proven within that system. Worse: such a system can never prove its own consistency. The dream of a complete, self-verifying mathematics—championed by Hilbert and the formalists—was shattered.

Gödel didn’t prove that math was broken. He proved it was \textit{bounded}. Any formal structure that could encode arithmetic would necessarily leave some truths outside its reach. Like a spotlight in a vast, dark room, logic could illuminate only so much.

\medskip

\begin{quote}
\textit{Truth, Gödel showed, is larger than proof.}
\end{quote}

\medskip

Decades later, \textbf{Gregory Chaitin} offered a modern extension of this idea—not with paradoxes, but with complexity. Building on the notion of \textbf{Kolmogorov complexity}, Chaitin argued that certain mathematical facts are \emph{true for no reason}—they cannot be derived from simpler principles because they are, in a precise sense, irreducibly complex.

At the center of his work is a striking question:

\begin{quote}
\emph{Why should I believe in a real number if I can’t calculate it, can’t prove what its digits are, and can’t even refer to it?}
\end{quote}

He argued that some real numbers—and, by analogy, some mathematical facts—are so intricate that they resist all formalization. Like pure noise, their structure contains no pattern, no compression, no proof shorter than the fact itself. These are not just complex—they are unprovable.

\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title={Sidebar: Chaitin, Gödel, and the Irreducible Truths of Mathematics}]
    \textbf{Kurt Gödel} showed that no formal system can capture all mathematical truths. \textbf{Gregory Chaitin} extended this by showing that some truths are unprovable because they are incompressible. Using tools from information theory, he introduced the idea that certain mathematical facts are true simply because they are too random to be derived.

    His concept of \emph{algorithmic randomness} treats some numbers like natural phenomena: we can observe their behavior, but no system of axioms will ever explain them. They are what they are, and no proof can make them otherwise.

    Chaitin’s work suggests that:
    \begin{itemize}
        \item Some theorems are unprovable not because of logical complexity, but because of informational density.
        \item Gödel’s incompleteness is not rare or pathological—it is pervasive.
        \item Mathematics, far from being a closed book, contains chapters that no reader can ever reach.
    \end{itemize}
\end{tcolorbox}


\subsection{An Example of Incompressible Randomness}

To grasp what it means for randomness to be so complete that it admits \emph{no compression or regularity}, consider the humble binary string: a sequence of 0s and 1s.

Suppose we have the following 64-bit string:

\begin{quote}
\texttt{0101010101010101010101010101010101010101010101010101010101010101}
\end{quote}

This pattern is immediately obvious. You could describe it to a friend as ``alternating 0s and 1s, repeated 32 times.'' That description is far shorter than the string itself. In algorithmic terms, you could write a very short program that outputs this exact string.

Now compare that to another 64-bit string:

\begin{quote}
\texttt{0110011001111001010001101010110001100101111001010110110010100101}
\end{quote}

At a glance, this one \emph{appears} random. No obvious pattern, no repetition, no symmetry. But here’s the crucial point: appearances can be deceiving. To formally say that a string is random, we require something stronger:

\begin{center}
\textbf{A string is algorithmically random if the shortest program that can output it is \emph{as long as the string itself}.}
\end{center}

This is the heart of \textbf{Kolmogorov complexity}. If no compression exists—no shortcut, no pattern, no structure—then the string is said to be \textbf{incompressible}. You cannot summarize it. You cannot reduce it. It is its own explanation.

These kinds of strings do exist. In fact, \emph{most} strings of sufficient length are incompressible. They just look like noise. There is no shorter description than the string itself. And that is what Chaitin calls \textbf{true randomness}.

\medskip

\begin{quote}
\textit{“True randomness is not the absence of order—it is the absence of compressible structure.”}
\end{quote}

\medskip

This idea leads directly to Chaitin’s interpretation of Gödel’s incompleteness: in any formal system, there will be facts—like specific bits of certain numbers—that are true but unprovable, precisely because they are incompressible. No derivation exists. No pattern explains them. They just are.

In practical terms, this is not just a curiosity—it matters for encryption, for complexity theory, and for understanding the limits of scientific modeling. Incompressibility means that no theory, no model, no machine can do better than brute force.

Some truths, like some binary strings, carry no message but their own existence.

\begin{tcolorbox}[colback=gray!5!white, colframe=black!75!white, title={Compression Test: A Thought Experiment}]
Imagine you're given a binary string with a million digits.

You try every data compression tool you know. ZIP. LZMA. Huffman coding. Every one of them outputs a file almost exactly the same size as the original.

What does this tell you?

Not that the string is meaningless—but that it contains no pattern exploitable by any algorithm. You have hit algorithmic randomness. You are staring at pure informational noise.

This is Chaitin’s world: a universe where some truths are unshrinkable.
\end{tcolorbox}


\subsection{The Boundary of Knowability}

These ideas challenge the very heart of formal reasoning. They don’t just tell us that some things are hard to prove—they tell us that some things \emph{cannot} be proven, not even in principle. The notion that mathematics could serve as a perfect mirror of truth collapses into a more nuanced picture: logic can only go so far, and beyond it lies an irreducible fog.

And yet, this boundary is not a dead end. It is a threshold.

In the sections that follow, we’ll see how this philosophical limit sets the stage for a new kind of reasoning—one that doesn’t merely ask what can be proven, but what can be \emph{carried out}. If Gödel taught us that truth outruns proof, the next question becomes:

\begin{center}
    \emph{What can be carried out mechanically, and what lies forever beyond the reach of procedure?}
\end{center}


\subsection{Compression, Search, and the Cost of Distinguishing}

To see how Kolmogorov complexity plays out in the real world, let’s revisit an earlier problem:  
the search for a single heavy marble among many, using a balance scale.

Each weighing gives you one of three outcomes: left heavier, right heavier, or equal.  
Each outcome gives you \(\log_2(3)\) bits of information — a ternary decision.

Now suppose you're handed a bag with \( n = 128 \) marbles.  
To find the heavy one, you need enough weighings to uniquely identify one item out of 128 possibilities.

That means gathering at least:
\[
\log_2(128) = 7 \text{ bits of information.}
\]

And since each weighing gives \( \log_2(3) \approx 1.584 \) bits,  
you need at least:
\[
\frac{7}{\log_2(3)} \approx 4.42 \Rightarrow 5 \text{ weighings (rounded up)}
\]

This isn't just a trick. It's a hard limit.

The best algorithm compresses the entire decision process into a tree with depth 5 — each branch encoding a ternary choice. This tree is like a program. If the marble’s position is compressible — say, always the 1st or 128th — the tree gets smaller. If not, you’re stuck with full complexity.

\begin{quote}
\textbf{The number of weighings is a proxy for the Kolmogorov complexity of the marble’s identity.}
\end{quote}

If your goal is to identify something from among \( n \) equally likely options,  
and your tool provides \( k \) outcomes per test,  
then:
\[
\text{Minimum steps} = \log_k(n)
\]

This is the same logic Chaitin used to argue that some facts are irreducibly complex:  
they require full-length, brute-force programs to express — no compression, no shortcuts, no cleverness.

Just like some marble arrangements require the full tree.

\begin{tcolorbox}[colback=gray!5!white, colframe=black, title=\textbf{Compression Is a Search Tree}]
Every time you distinguish one item from \( n \), you’re navigating a tree.  
The shorter the tree, the lower the Kolmogorov complexity.  
But when no structure exists, the tree grows — and you’re stuck with the full cost of separation.
\end{tcolorbox}



\subsection{Why Some Things Cannot Be Learned}

Learning is compression.

Whether you're fitting a model to data, optimizing a loss function, or training a neural network, you're looking for patterns — shortcuts that explain more with less. That’s what generalization is: recognizing a simple rule behind complex behavior.

But what if the rule doesn’t exist?

In Kolmogorov’s world, some binary strings cannot be compressed. Their shortest description is themselves. They look random — not because we haven’t figured them out yet, but because there’s nothing to figure out.

\bigskip

Now apply that idea to real-world systems.

\begin{itemize}
    \item Suppose you’re trying to model the behavior of a chaotic market.
    \item Or forecast human decision-making at scale.
    \item Or learn a physical system with hidden variables, entanglements, or extreme sensitivity to initial conditions.
\end{itemize}

If the system's behavior corresponds to an incompressible function — one whose output cannot be generated by any program shorter than itself — then no model can learn it.  
Not a neural network. Not a symbolic AI system. Not you.

\bigskip

\textbf{You could memorize it — but you cannot generalize it.}

This is the boundary between learning and memorization, between modeling and noise.  
It’s not just that some patterns are \emph{hard} to learn.  
It’s that some are \emph{fundamentally unlearnable}, because they contain no pattern at all.

\bigskip

\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title={Unlearnability in the Wild}]
When we ask a model to learn, we’re really asking:  
“Is there a compressed representation of this behavior?”

But Kolmogorov and Chaitin teach us that some behaviors are their own explanation.  
If the shortest program to simulate the world is the world itself,  
then no learning is possible.

Only observation. Or brute-force memorization.
\end{tcolorbox}

\bigskip

So yes — real things we want to learn may be unlearnable.

Not because we're too slow.  
Not because we lack data.  
But because the structure we hope to find... doesn't exist.


\subsection{The Three-Body Problem Isn’t Unsovable — It’s Unknowable}

There’s a persistent myth that the three-body problem has “no solution.” But that’s not quite right.

In both Newtonian and Lagrangian mechanics, a solution \emph{does} exist. Given initial conditions, there is a unique path the system will follow, dictated by the equations of motion. And in the Lagrangian formulation, the universe picks that path by minimizing the action:

\[
\delta \int_{t_0}^{t_1} L(q, \dot{q}, t) \, dt = 0
\]

The trajectory is real, determined, and physical.

But here’s the twist: \textbf{we cannot know it in any closed, compressed form}.

Unlike the two-body problem, where Kepler’s ellipses give us neat orbital equations, the three-body problem resists all attempts at symbolic summary. Poincaré showed it’s not algebraically integrable. Its behavior is chaotic, sensitive, and non-repeating.

And that’s not a numerical inconvenience.  
It’s an epistemological limit.

\bigskip

From the lens of \textbf{Kolmogorov complexity}, the solution path may be so sensitive, so entangled with initial conditions, that there exists no program shorter than the full simulation itself that can describe its behavior. The shortest explanation is: just run the system.

\bigskip

\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black, title={From Determinism to Incompressibility}]
The Lagrangian says: “There is one path the universe will take.”

Kolmogorov replies: “Yes — but you’ll never write it down.”

You can simulate the outcome.  
You can observe it.  
But you cannot \textbf{compress} it.
\end{tcolorbox}

\bigskip

This is what makes the three-body problem so profound.

It’s not a failure of physics.  
It’s a boundary in knowledge.

We know the solution exists.  
We just can’t express it.

Not because it’s hidden.  
But because it is, in a precise sense, \textbf{unlearnable}.
