\section{From Abstract Symmetries to Neural Matrices: Representation Theory in Deep Learning}

\subsection{Making the Abstract Concrete}

We have seen how cohomology extracts global invariants from geometry and physics, turning “holes” into algebraic data living in vector spaces.  In much the same spirit, \textbf{representation theory} transforms abstract algebraic structures—groups, algebras, category‐theoretic objects—into concrete linear operators on vector spaces.  And today, those very linear operators power the engines of deep learning.

In many applied fields, an abstract operation or symmetry is rendered as a concrete linear operator—often a matrix or block of matrices—so that standard linear-algebra techniques can be deployed.

\subsection{Example: Fourier Multipliers}  
A convolution operator \(T\) on signals,
\[
(Tf)(t)\;=\;\int_{\mathbb R}k(t-s)\,f(s)\,ds,
\]
is originally defined by an integral “law.”  Under the Fourier transform it becomes
\[
\widehat{Tf}(\omega)
=\;\widehat{k}(\omega)\,\widehat{f}(\omega),
\]
i.e.\ a \emph{diagonal} multiplication operator in the frequency basis.  What once was an integral kernel is now a matrix of scalars \(\widehat{k}(\omega)\), and we immediately read off:
\[
\text{eigenfunctions}\;e^{i\omega t},\quad
\text{eigenvalues}\;\widehat{k}(\omega),
\]
enabling norm estimates, low-rank approximations, and spectral filtering by ordinary pointwise algebra.


\paragraph{Symbol Classes and Operator Norms}
By the Plancherel theorem, for any \(f\in L^2(\mathbb R)\),
\[
\|Tf\|_{L^2}
= \|\widehat{Tf}\|_{L^2}
= \bigl\|\,\widehat{k}(\omega)\,\widehat{f}(\omega)\bigr\|_{L^2}
\le \|\widehat{k}\|_{L^\infty}\,\|\widehat{f}\|_{L^2}
= \|\widehat{k}\|_{L^\infty}\,\|f\|_{L^2}.
\]
Hence
\[
\|T\|_{L^2\to L^2}
= \|\widehat{k}\|_{L^\infty}
\]
and the “matrix of scalars” \(\widehat{k}(\omega)\) acts exactly like the diagonal entries of a bounded operator on \(\ell^2\).

\paragraph{Mikhlin Multipliers on \(L^p\)}
More generally, if the symbol \(m(\omega)=\widehat{k}(\omega)\) satisfies the Mikhlin condition
\[
\sup_{\omega\neq0}\,|\omega|^j\bigl|m^{(j)}(\omega)\bigr|\;<\;\infty
\quad\text{for }0\le j\le \bigl\lfloor\tfrac n2\bigr\rfloor+1,
\]
then \(T\) extends to a bounded operator on \(L^p(\mathbb R^n)\) for all \(1<p<\infty\).  Convolution kernels with sufficiently rapid decay (Schwartz class) automatically produce smooth multipliers \(m(\omega)\), guaranteeing stability across norms.

\paragraph{Fractional Laplacian and Sobolev Spaces}
Choosing the symbol
\[
m(\omega)=|\omega|^{2s},
\]
we recover the fractional Laplacian
\[
(-\Delta)^s f(x)
=\mathcal F^{-1}\bigl(|\omega|^{2s}\,\widehat f(\omega)\bigr),
\]
which defines an isomorphism
\[
(-\Delta)^s: H^s(\mathbb R^n)\;\xrightarrow{\;\cong\;}\;H^{-s}(\mathbb R^n).
\]
Here the Sobolev norm
\(\|f\|_{H^s}^2=\int(1+|\omega|^2)^s|\widehat f(\omega)|^2\,d\omega\)
is itself a Fourier multiplier norm, illustrating how function‐space geometry is encoded by diagonal matrices in frequency space.

\paragraph{Pseudodifferential Perspectives}
Going beyond polynomial symbols, one defines a pseudodifferential operator
\[
(Tf)(x)
=\int_{\mathbb R^n}e^{i x\cdot\omega}\,a(x,\omega)\,\widehat f(\omega)\,d\omega,
\]
where \(a(x,\omega)\) is a symbol of order \(m\).  Under suitable symbol classes \(S^m_{\rho,\delta}\), these operators inherit mapping properties between Sobolev spaces that mirror classical matrix calculus—showing once more that even the most abstract integral laws become “just” weighted diagonal actions after passage to the Fourier side.








\subsection{Example: Linear Control Systems}  
A linear time-invariant system
\[
\dot x(t) \;=\; A\,x(t) \;+\; B\,u(t),\qquad
y(t)=C\,x(t)+D\,u(t)
\]
is encoded by the state matrix \(A\in\mathbb R^{n\times n}\).  Its flow
\[
x(t)=e^{At}x(0)
\]
is computed via the matrix exponential, and the transfer function
\[
H(s)\;=\;C\,(sI - A)^{-1}B \;+\; D
\]
turns differentiation into the algebraic inversion of \((sI-A)\).  Questions of stability, controllability, and resonance reduce to:
\[
\text{eigenvalues of }A,\quad
\text{singular values of }(sI-A)^{-1},\quad
\text{rank}\bigl[\!B,\,AB,\,\dots\bigr],
\]
all standard linear-algebra routines.


\paragraph{Spectral Criteria for Stability}  
By the spectral mapping theorem, the eigenvalues \(\{\lambda_i\}\) of \(A\) determine the growth or decay of the flow:
\[
\sigma(e^{At}) = \{e^{\lambda_i t}\}\,.
\]
In particular, the system is asymptotically stable (i.e.\ \(x(t)\to0\) for all \(x(0)\)) if and only if
\[
\Re(\lambda_i)<0\quad\forall i.
\]
Moreover, one shows there exists constants \(M,\alpha>0\) so that
\[
\|e^{At}\|\;\le\;M\,e^{\alpha t},\qquad
\alpha = \max_i \Re(\lambda_i),
\]
turning a transcendental flow into simple exponential bounds.

\paragraph{Controllability and the Gramian}  
The \emph{controllability Gramian}
\[
W_c \;=\;\int_{0}^{\infty} e^{At}\,B\,B^{T}\,e^{A^{T}t}\,dt
\]
satisfies the Lyapunov equation
\[
A\,W_c + W_c\,A^T + B\,B^T = 0.
\]
By standard linear algebra, the system is controllable if and only if \(W_c\) is invertible (full rank).  Computing its eigenvalues and condition number reduces controllability questions to matrix–factorization routines.

\paragraph{Observability and Duality}  
Dually, the \emph{observability Gramian}
\[
W_o \;=\;\int_{0}^{\infty} e^{A^{T}t}\,C^T\,C\,e^{At}\,dt
\]
solves
\[
A^T\,W_o + W_o\,A + C^T\,C = 0,
\]
and the pair \((C,A)\) is observable precisely if \(W_o\) is nonsingular.  Thus, observability also becomes a question of nullspaces and eigenstructure of explicit matrices.

\paragraph{\(H_{2}\) and \(H_{\infty}\) Norms}  
The energy gain from input to output can be quantified by the \(H_2\) norm
\[
\|H\|_{H_2}^2
= \frac{1}{2\pi}\int_{-\infty}^{\infty}\!\!\|H(i\omega)\|_F^2\,d\omega
=\operatorname{trace}\bigl(C\,W_c\,C^T\bigr),
\]
where \(\|\cdot\|_F\) is the Frobenius norm.  The worst–case amplification is given by the \(H_\infty\) norm
\[
\|H\|_{H_\infty}
= \sup_{\omega\in\mathbb R}\sigma_{\max}\bigl(H(i\omega)\bigr)
=\sup_{\omega}\,\bigl\|C\,(i\omega I - A)^{-1}B + D\bigr\|_{2},
\]
which can be estimated via singular‐value routines on the frequency samples \((i\omega I - A)^{-1}\).

\paragraph{Riccati Equations and Optimal Control}  
In the Linear–Quadratic Regulator problem, one solves the algebraic Riccati equation
\[
A^T P + P\,A - P\,B\,R^{-1}B^T P + Q = 0
\]
for \(P\succ0\).  Here \(Q\) and \(R\) weight state and input penalties.  The optimal feedback law \(u = -R^{-1}B^T P\,x\) minimizes a quadratic cost, and all steps—solving the Riccati equation, computing gain matrices, and evaluating closed‐loop eigenvalues—are standard linear‐algebra computations.

\paragraph{Model Reduction via Balanced Truncation}  
Using the Gramians \(W_c,W_o\), one forms a balancing transformation that equalizes their eigenvalues (the Hankel singular values).  Truncating small modes yields a reduced model \(\hat A,\hat B,\hat C,\hat D\) whose error \(\|H-\hat H\|_{H_\infty}\) is bounded by twice the sum of discarded singular values.  This entire procedure rests on SVD and similarity transformations.

\medskip

In every case—whether assessing stability, designing controllers, or reducing model order—the infinite‐dimensional dynamics and differential laws are “made concrete” as matrix equations.  We then deploy eigenanalysis, Lyapunov solvers, Riccati factorizations, and SVD, turning abstract control concepts into algorithms built on universally understood linear algebra.  















\subsection{Example: Cellular Cohomology}  
A topological space can be broken into simplices (vertices, edges, faces, \dots) giving chain groups \(C_k\cong\Bbb R^{n_k}\).  The boundary maps
\[
\partial_k: C_k \;\longrightarrow\; C_{k-1}
\]
become explicit incidence matrices.  Passing to cochains,
\[
\delta^{k} = \partial_{k+1}^{T}: C^{k} \;\longrightarrow\; C^{k+1},
\]
we define
\[
H^k \;=\;\frac{\ker \delta^k}{\operatorname{im}\delta^{k-1}}.
\]
For instance, on a simplicial circle with vertices \(v_1,v_2\) and edge \(e\), the boundary matrix is
\[
\partial_1 = \begin{pmatrix}1 & -1\end{pmatrix},
\]
so \(\delta^0=\partial_1^T\) and one computes
\(\dim H^0=1,\ \dim H^1=1\)
by nullspace and image dimensions.  Thus Betti numbers arise from rank-nullity of small matrices.

\paragraph{Rank–Nullity and Betti Numbers}  
In general, if \(C_k\cong\mathbb R^{n_k}\) and \(\partial_k\) has rank \(r_k\), then by rank–nullity
\[
\dim\ker\partial_k = n_k - r_k,\qquad
\dim\operatorname{im}\partial_{k+1} = r_{k+1}.
\]
Since \(\delta^{k-1}=\partial_k^T\), one finds
\[
\dim H^k
= \dim\ker\delta^k - \dim\operatorname{im}\delta^{k-1}
= (n_k - r_{k+1}) - r_k,
\]
so the \(k\)th Betti number \(\beta_k\) is
\[
\beta_k = n_k - r_k - r_{k+1}.
\]
Thus computing cohomology reduces entirely to computing the ranks of the sparse matrices \(\partial_k\).

\paragraph{Example: The 2–Sphere via Cells}  
Take the standard CW–decomposition of \(S^2\) with one 0–cell \(v\), no 1–cells, and one 2–cell \(f\).  Here
\[
n_0=1,\quad n_1=0,\quad n_2=1,
\]
and both boundary maps \(\partial_1\) and \(\partial_2\) are the zero matrix.  Hence
\[
r_1 = \operatorname{rank}(\partial_1) = 0,\quad
r_2 = \operatorname{rank}(\partial_2)=0,
\]
and
\(\beta_0 = 1-0-0 = 1,\quad \beta_1 = 0-0-0=0,\quad \beta_2 = 1-0-0=1.\)

\paragraph{Discrete Hodge Theory}  
Define the \emph{combinatorial Laplacian} on \(k\)–cochains by
\[
\Delta^k = \delta^{k-1}\,\delta^{k-1\,T} \;+\;\delta^k{}^{T}\,\delta^k.
\]
One shows that
\[
\ker\Delta^k \;\cong\;H^k,
\]
so the Betti number \(\beta_k\) also equals the multiplicity of the zero eigenvalue of \(\Delta^k\).  In practice, sparse eigensolvers on \(\Delta^k\) recover harmonic cochains and give a basis for \(H^k\).

\paragraph{Persistent Cohomology}  
Given a filtered complex
\(\emptyset = K_0 \subset K_1 \subset \cdots \subset K_m\),
one tracks the boundary matrices \(\partial_k^{(i)}\) at each stage and performs column‐reduction to compute \emph{persistence pairs} \((b,d)\).  Concretely, one maintains a reduced matrix \(R\) so that each new simplex either creates a new cohomology class (a “birth”) or kills an existing one (a “death”).  The resulting barcode \(\{[b_j,d_j)\}\) summarizes the scales at which topological features appear and vanish.

\paragraph{Cup Product and Cohomology Ring}  
On cochains, the cup product
\[
\smile \;:\; C^p\times C^q \;\longrightarrow\; C^{p+q}
\]
is realized by combinatorial rules on simplices, and at the matrix level one assembles a bilinear map
\(\smile: \mathbb R^{n_p}\times\mathbb R^{n_q}\to\mathbb R^{n_{p+q}}\).  
After passing to cohomology, this endows \(H^*(K)\) with the structure of a graded–commutative algebra.  Computing representatives of cup–products reduces to simple matrix multiplications and projections onto harmonic subspaces.

\medskip  
Together, these constructions show that cohomology—far from being a purely theoretical tool—boils down to sparse linear‐algebra operations: rank computations, nullspace extractions, eigendecompositions, and matrix reductions.  The abstract notion of “holes” becomes an explicit task in numerical linear algebra.  



























\subsection{Linear Algebra as the Engine of Modern Learning}
Deep learning at its core is \emph{linear algebra at scale}:
\begin{itemize}
  \item \textbf{Layers as linear maps:} each fully‐connected or convolutional layer applies a weight matrix \(W\) (often millions of parameters) to an input vector or tensor.
  \item \textbf{Activations and feature spaces:} nonlinearity \(f\) (ReLU, sigmoid, etc.) sits between linear steps, but the heavy lifting—dimensionality reduction, feature extraction, manifold unfolding—happens via matrix multiplications and tensor contractions.
  \item \textbf{Backpropagation:} gradients flow through these linear maps via transposes \(W^\top\) and chain‐rule assemblies, again exploiting the simplicity of matrix‐vector calculus.
  \item \textbf{Optimization:} algorithms like SGD, Adam, or second‐order methods repeatedly solve linear subproblems or compute Jacobian–vector products, all relying on fast linear algebra kernels on GPUs or TPUs.
\end{itemize}

Thus, what used to be pen‐and‐paper derivations of abstract symmetries or group actions can now be \emph{encoded} as weight matrices and \emph{optimized} by computers—bringing the evolution of mathematics from Pythagoras’s ratios of a triangle to today’s trillion‐parameter networks full circle.





\subsection{Representation Theory Meets Neural Architectures}
Beyond raw matrix algebra, representation theory informs modern architectures:
\begin{itemize}
  \item \textbf{Equivariance and symmetry:} by choosing weight tensors that respect a group action (e.g.\ rotations \(\mathrm{SO}(2)\), permutations \(S_n\)), one builds \emph{equivariant neural networks} whose learned functions naturally obey the same invariances as the data.
  \item \textbf{Harmonic analysis on graphs and manifolds:} graph convolutional networks and geometric deep learning use the representation theory of discrete Laplacians or Lie groups to generalize CNNs to non-Euclidean domains.
  \item \textbf{Tensor factorization and decomposition:} higher‐order representations guide the design of low-rank tensor layers, compressing models while preserving key algebraic features.
\end{itemize}

In each case, the abstract concept—whether a group of symmetries or an algebra of operators—is realized concretely as arrays of numbers.  Through the hardware‐accelerated linear algebra primitives of modern computing, we can now manipulate, optimize, and discover patterns that would have been intractable mere decades ago.





\subsection{A New Chapter in Mathematical Evolution}
From the Pythagorean discovery of numerical harmony in geometry, through the abstraction of cohomology and group representations, to today’s deep neural networks, the trajectory is clear:

\begin{quote}
\textit{What was once carved on tablets and proven with pen and paper is now instantiated in code and optimized by silicon.}
\end{quote}

Computers do not replace mathematical insight—they amplify it.  By encoding abstract algebraic objects as matrices and tensors, deep learning frameworks stand on the shoulders of centuries of mathematical evolution, making the once–impractical concrete, and enabling humans to explore ever more complex patterns at scale.

